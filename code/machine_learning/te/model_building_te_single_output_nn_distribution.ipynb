{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Week</th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "      <th>Team</th>\n",
       "      <th>Fantasy Points</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Position Rank</th>\n",
       "      <th>Adjusted Passing Yards Projection</th>\n",
       "      <th>Adjusted Passing Touchdowns Projection</th>\n",
       "      <th>Adjusted Interceptions Projection</th>\n",
       "      <th>Adjusted Rushing Yards Projection</th>\n",
       "      <th>Adjusted Receiving Yards Projection</th>\n",
       "      <th>Adjusted Receptions Projection</th>\n",
       "      <th>Anytime Touchdown Probability</th>\n",
       "      <th>Location</th>\n",
       "      <th>Team Projected Score</th>\n",
       "      <th>Opponent Projected Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>george kittle</td>\n",
       "      <td>TE</td>\n",
       "      <td>SF</td>\n",
       "      <td>7.3</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>6.464916</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.75</td>\n",
       "      <td>20.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>travis kelce</td>\n",
       "      <td>TE</td>\n",
       "      <td>KC</td>\n",
       "      <td>14.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.524775</td>\n",
       "      <td>5.557539</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.50</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>mark andrews</td>\n",
       "      <td>TE</td>\n",
       "      <td>BAL</td>\n",
       "      <td>20.3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>4.436640</td>\n",
       "      <td>0.420168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.25</td>\n",
       "      <td>20.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>zach ertz</td>\n",
       "      <td>TE</td>\n",
       "      <td>PHI</td>\n",
       "      <td>9.3</td>\n",
       "      <td>76.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>5.458984</td>\n",
       "      <td>0.380228</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>16.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>darren waller</td>\n",
       "      <td>TE</td>\n",
       "      <td>LV</td>\n",
       "      <td>7.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>4.544513</td>\n",
       "      <td>0.420168</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>25.50</td>\n",
       "      <td>22.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>mike gesicki</td>\n",
       "      <td>TE</td>\n",
       "      <td>NE</td>\n",
       "      <td>5.5</td>\n",
       "      <td>197.0</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.505537</td>\n",
       "      <td>2.431481</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.75</td>\n",
       "      <td>27.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>josh oliver</td>\n",
       "      <td>TE</td>\n",
       "      <td>MIN</td>\n",
       "      <td>2.2</td>\n",
       "      <td>200.0</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.75</td>\n",
       "      <td>20.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>austin hooper</td>\n",
       "      <td>TE</td>\n",
       "      <td>LV</td>\n",
       "      <td>4.3</td>\n",
       "      <td>217.0</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.503353</td>\n",
       "      <td>2.462766</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>19.50</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>adam trautman</td>\n",
       "      <td>TE</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>225.0</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.600127</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.50</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>tommy tremble</td>\n",
       "      <td>TE</td>\n",
       "      <td>CAR</td>\n",
       "      <td>1.1</td>\n",
       "      <td>226.0</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.511851</td>\n",
       "      <td>1.612455</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>17.25</td>\n",
       "      <td>20.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2015 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Season  Week           Name Position Team  Fantasy Points   Rank  \\\n",
       "0       2020     1  george kittle       TE   SF             7.3   43.0   \n",
       "1       2020     1   travis kelce       TE   KC            14.0   48.0   \n",
       "2       2020     1   mark andrews       TE  BAL            20.3   70.0   \n",
       "3       2020     1      zach ertz       TE  PHI             9.3   76.0   \n",
       "4       2020     1  darren waller       TE   LV             7.5   87.0   \n",
       "...      ...   ...            ...      ...  ...             ...    ...   \n",
       "2010    2023    17   mike gesicki       TE   NE             5.5  197.0   \n",
       "2011    2023    17    josh oliver       TE  MIN             2.2  200.0   \n",
       "2012    2023    17  austin hooper       TE   LV             4.3  217.0   \n",
       "2013    2023    17  adam trautman       TE  DEN             1.4  225.0   \n",
       "2014    2023    17  tommy tremble       TE  CAR             1.1  226.0   \n",
       "\n",
       "      Position Rank  Adjusted Passing Yards Projection  \\\n",
       "0                 1                                NaN   \n",
       "1                 2                                NaN   \n",
       "2                 3                                NaN   \n",
       "3                 4                                NaN   \n",
       "4                 5                                NaN   \n",
       "...             ...                                ...   \n",
       "2010             31                                NaN   \n",
       "2011             32                                NaN   \n",
       "2012             34                                NaN   \n",
       "2013             35                                NaN   \n",
       "2014             36                                NaN   \n",
       "\n",
       "      Adjusted Passing Touchdowns Projection  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        NaN   \n",
       "4                                        NaN   \n",
       "...                                      ...   \n",
       "2010                                     NaN   \n",
       "2011                                     NaN   \n",
       "2012                                     NaN   \n",
       "2013                                     NaN   \n",
       "2014                                     NaN   \n",
       "\n",
       "      Adjusted Interceptions Projection  Adjusted Rushing Yards Projection  \\\n",
       "0                                   NaN                                NaN   \n",
       "1                                   NaN                                NaN   \n",
       "2                                   NaN                                NaN   \n",
       "3                                   NaN                                NaN   \n",
       "4                                   NaN                                NaN   \n",
       "...                                 ...                                ...   \n",
       "2010                                NaN                                NaN   \n",
       "2011                                NaN                                NaN   \n",
       "2012                                NaN                                NaN   \n",
       "2013                                NaN                                NaN   \n",
       "2014                                NaN                                NaN   \n",
       "\n",
       "      Adjusted Receiving Yards Projection  Adjusted Receptions Projection  \\\n",
       "0                               71.500000                        6.464916   \n",
       "1                               69.524775                        5.557539   \n",
       "2                               48.500000                        4.436640   \n",
       "3                               54.500000                        5.458984   \n",
       "4                               52.500000                        4.544513   \n",
       "...                                   ...                             ...   \n",
       "2010                            17.505537                        2.431481   \n",
       "2011                            10.500000                             NaN   \n",
       "2012                            20.503353                        2.462766   \n",
       "2013                             5.500000                        0.600127   \n",
       "2014                            13.511851                        1.612455   \n",
       "\n",
       "      Anytime Touchdown Probability  Location  Team Projected Score  \\\n",
       "0                          0.500000       1.0                 26.75   \n",
       "1                          0.545455       1.0                 31.50   \n",
       "2                          0.420168       1.0                 27.25   \n",
       "3                          0.380228      -1.0                 22.00   \n",
       "4                          0.420168      -1.0                 25.50   \n",
       "...                             ...       ...                   ...   \n",
       "2010                       0.166667      -1.0                 12.75   \n",
       "2011                       0.181818       1.0                 21.75   \n",
       "2012                       0.181818      -1.0                 19.50   \n",
       "2013                       0.200000       1.0                 21.50   \n",
       "2014                       0.166667      -1.0                 17.25   \n",
       "\n",
       "      Opponent Projected Score  \n",
       "0                        20.25  \n",
       "1                        22.00  \n",
       "2                        20.25  \n",
       "3                        16.50  \n",
       "4                        22.50  \n",
       "...                        ...  \n",
       "2010                     27.25  \n",
       "2011                     20.75  \n",
       "2012                     23.00  \n",
       "2013                     18.00  \n",
       "2014                     20.75  \n",
       "\n",
       "[2015 rows x 18 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "df_mod = pd.read_parquet('../../../data/model_data/model_data_single_output.parquet')\n",
    "\n",
    "df_mod = df_mod.loc[df_mod['Position'] == 'TE', :].reset_index(drop=True)\n",
    "\n",
    "df_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Position Rank</th>\n",
       "      <th>Location</th>\n",
       "      <th>Team Projected Score</th>\n",
       "      <th>Opponent Projected Score</th>\n",
       "      <th>Adjusted Receptions Projection</th>\n",
       "      <th>Adjusted Receiving Yards Projection</th>\n",
       "      <th>Anytime Touchdown Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.75</td>\n",
       "      <td>20.25</td>\n",
       "      <td>6.464916</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.50</td>\n",
       "      <td>22.00</td>\n",
       "      <td>5.557539</td>\n",
       "      <td>69.524775</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.25</td>\n",
       "      <td>20.25</td>\n",
       "      <td>4.436640</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>0.420168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>16.50</td>\n",
       "      <td>5.458984</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>0.380228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>25.50</td>\n",
       "      <td>22.50</td>\n",
       "      <td>4.544513</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>0.420168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>197.0</td>\n",
       "      <td>31</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.75</td>\n",
       "      <td>27.25</td>\n",
       "      <td>2.431481</td>\n",
       "      <td>17.505537</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>200.0</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.75</td>\n",
       "      <td>20.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>217.0</td>\n",
       "      <td>34</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>19.50</td>\n",
       "      <td>23.00</td>\n",
       "      <td>2.462766</td>\n",
       "      <td>20.503353</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>225.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.50</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.600127</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>226.0</td>\n",
       "      <td>36</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>17.25</td>\n",
       "      <td>20.75</td>\n",
       "      <td>1.612455</td>\n",
       "      <td>13.511851</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2015 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rank  Position Rank  Location  Team Projected Score  \\\n",
       "0      43.0              1       1.0                 26.75   \n",
       "1      48.0              2       1.0                 31.50   \n",
       "2      70.0              3       1.0                 27.25   \n",
       "3      76.0              4      -1.0                 22.00   \n",
       "4      87.0              5      -1.0                 25.50   \n",
       "...     ...            ...       ...                   ...   \n",
       "2010  197.0             31      -1.0                 12.75   \n",
       "2011  200.0             32       1.0                 21.75   \n",
       "2012  217.0             34      -1.0                 19.50   \n",
       "2013  225.0             35       1.0                 21.50   \n",
       "2014  226.0             36      -1.0                 17.25   \n",
       "\n",
       "      Opponent Projected Score  Adjusted Receptions Projection  \\\n",
       "0                        20.25                        6.464916   \n",
       "1                        22.00                        5.557539   \n",
       "2                        20.25                        4.436640   \n",
       "3                        16.50                        5.458984   \n",
       "4                        22.50                        4.544513   \n",
       "...                        ...                             ...   \n",
       "2010                     27.25                        2.431481   \n",
       "2011                     20.75                             NaN   \n",
       "2012                     23.00                        2.462766   \n",
       "2013                     18.00                        0.600127   \n",
       "2014                     20.75                        1.612455   \n",
       "\n",
       "      Adjusted Receiving Yards Projection  Anytime Touchdown Probability  \n",
       "0                               71.500000                       0.500000  \n",
       "1                               69.524775                       0.545455  \n",
       "2                               48.500000                       0.420168  \n",
       "3                               54.500000                       0.380228  \n",
       "4                               52.500000                       0.420168  \n",
       "...                                   ...                            ...  \n",
       "2010                            17.505537                       0.166667  \n",
       "2011                            10.500000                       0.181818  \n",
       "2012                            20.503353                       0.181818  \n",
       "2013                             5.500000                       0.200000  \n",
       "2014                            13.511851                       0.166667  \n",
       "\n",
       "[2015 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_mod[[\n",
    "    'Rank',\n",
    "    'Position Rank',\n",
    "    'Location',\n",
    "    'Team Projected Score',\n",
    "    'Opponent Projected Score',\n",
    "    'Adjusted Receptions Projection',\n",
    "    'Adjusted Receiving Yards Projection',\n",
    "    'Anytime Touchdown Probability',\n",
    "]].copy()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        7\n",
       "1       14\n",
       "2       20\n",
       "3        9\n",
       "4        8\n",
       "        ..\n",
       "2010     6\n",
       "2011     2\n",
       "2012     4\n",
       "2013     1\n",
       "2014     1\n",
       "Name: Fantasy Points, Length: 2015, dtype: int32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_rounded = df_mod['Fantasy Points'].round().astype(int)\n",
    "\n",
    "points_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-4 Fantasy Points</th>\n",
       "      <th>-3 Fantasy Points</th>\n",
       "      <th>-2 Fantasy Points</th>\n",
       "      <th>-1 Fantasy Points</th>\n",
       "      <th>0 Fantasy Points</th>\n",
       "      <th>1 Fantasy Points</th>\n",
       "      <th>2 Fantasy Points</th>\n",
       "      <th>3 Fantasy Points</th>\n",
       "      <th>4 Fantasy Points</th>\n",
       "      <th>5 Fantasy Points</th>\n",
       "      <th>6 Fantasy Points</th>\n",
       "      <th>7 Fantasy Points</th>\n",
       "      <th>8 Fantasy Points</th>\n",
       "      <th>9 Fantasy Points</th>\n",
       "      <th>10 Fantasy Points</th>\n",
       "      <th>11 Fantasy Points</th>\n",
       "      <th>12 Fantasy Points</th>\n",
       "      <th>13 Fantasy Points</th>\n",
       "      <th>14 Fantasy Points</th>\n",
       "      <th>15 Fantasy Points</th>\n",
       "      <th>16 Fantasy Points</th>\n",
       "      <th>17 Fantasy Points</th>\n",
       "      <th>18 Fantasy Points</th>\n",
       "      <th>19 Fantasy Points</th>\n",
       "      <th>20 Fantasy Points</th>\n",
       "      <th>21 Fantasy Points</th>\n",
       "      <th>22 Fantasy Points</th>\n",
       "      <th>23 Fantasy Points</th>\n",
       "      <th>24 Fantasy Points</th>\n",
       "      <th>25 Fantasy Points</th>\n",
       "      <th>26 Fantasy Points</th>\n",
       "      <th>27 Fantasy Points</th>\n",
       "      <th>28 Fantasy Points</th>\n",
       "      <th>29 Fantasy Points</th>\n",
       "      <th>30 Fantasy Points</th>\n",
       "      <th>31 Fantasy Points</th>\n",
       "      <th>32 Fantasy Points</th>\n",
       "      <th>33 Fantasy Points</th>\n",
       "      <th>34 Fantasy Points</th>\n",
       "      <th>35 Fantasy Points</th>\n",
       "      <th>36 Fantasy Points</th>\n",
       "      <th>37 Fantasy Points</th>\n",
       "      <th>38 Fantasy Points</th>\n",
       "      <th>39 Fantasy Points</th>\n",
       "      <th>40 Fantasy Points</th>\n",
       "      <th>41 Fantasy Points</th>\n",
       "      <th>42 Fantasy Points</th>\n",
       "      <th>43 Fantasy Points</th>\n",
       "      <th>44 Fantasy Points</th>\n",
       "      <th>45 Fantasy Points</th>\n",
       "      <th>46 Fantasy Points</th>\n",
       "      <th>47 Fantasy Points</th>\n",
       "      <th>48 Fantasy Points</th>\n",
       "      <th>49 Fantasy Points</th>\n",
       "      <th>50 Fantasy Points</th>\n",
       "      <th>51 Fantasy Points</th>\n",
       "      <th>52 Fantasy Points</th>\n",
       "      <th>53 Fantasy Points</th>\n",
       "      <th>54 Fantasy Points</th>\n",
       "      <th>55 Fantasy Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2015 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      -4 Fantasy Points  -3 Fantasy Points  -2 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      -1 Fantasy Points  0 Fantasy Points  1 Fantasy Points  2 Fantasy Points  \\\n",
       "0                   0.0               0.0               0.0               0.0   \n",
       "1                   0.0               0.0               0.0               0.0   \n",
       "2                   0.0               0.0               0.0               0.0   \n",
       "3                   0.0               0.0               0.0               0.0   \n",
       "4                   0.0               0.0               0.0               0.0   \n",
       "...                 ...               ...               ...               ...   \n",
       "2010                0.0               0.0               0.0               0.0   \n",
       "2011                0.0               0.0               0.0               1.0   \n",
       "2012                0.0               0.0               0.0               0.0   \n",
       "2013                0.0               0.0               1.0               0.0   \n",
       "2014                0.0               0.0               1.0               0.0   \n",
       "\n",
       "      3 Fantasy Points  4 Fantasy Points  5 Fantasy Points  6 Fantasy Points  \\\n",
       "0                  0.0               0.0               0.0               0.0   \n",
       "1                  0.0               0.0               0.0               0.0   \n",
       "2                  0.0               0.0               0.0               0.0   \n",
       "3                  0.0               0.0               0.0               0.0   \n",
       "4                  0.0               0.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2010               0.0               0.0               0.0               1.0   \n",
       "2011               0.0               0.0               0.0               0.0   \n",
       "2012               0.0               1.0               0.0               0.0   \n",
       "2013               0.0               0.0               0.0               0.0   \n",
       "2014               0.0               0.0               0.0               0.0   \n",
       "\n",
       "      7 Fantasy Points  8 Fantasy Points  9 Fantasy Points  10 Fantasy Points  \\\n",
       "0                  1.0               0.0               0.0                0.0   \n",
       "1                  0.0               0.0               0.0                0.0   \n",
       "2                  0.0               0.0               0.0                0.0   \n",
       "3                  0.0               0.0               1.0                0.0   \n",
       "4                  0.0               1.0               0.0                0.0   \n",
       "...                ...               ...               ...                ...   \n",
       "2010               0.0               0.0               0.0                0.0   \n",
       "2011               0.0               0.0               0.0                0.0   \n",
       "2012               0.0               0.0               0.0                0.0   \n",
       "2013               0.0               0.0               0.0                0.0   \n",
       "2014               0.0               0.0               0.0                0.0   \n",
       "\n",
       "      11 Fantasy Points  12 Fantasy Points  13 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      14 Fantasy Points  15 Fantasy Points  16 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   1.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      17 Fantasy Points  18 Fantasy Points  19 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      20 Fantasy Points  21 Fantasy Points  22 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   1.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      23 Fantasy Points  24 Fantasy Points  25 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      26 Fantasy Points  27 Fantasy Points  28 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      29 Fantasy Points  30 Fantasy Points  31 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      32 Fantasy Points  33 Fantasy Points  34 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      35 Fantasy Points  36 Fantasy Points  37 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      38 Fantasy Points  39 Fantasy Points  40 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      41 Fantasy Points  42 Fantasy Points  43 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      44 Fantasy Points  45 Fantasy Points  46 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      47 Fantasy Points  48 Fantasy Points  49 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      50 Fantasy Points  51 Fantasy Points  52 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2010                0.0                0.0                0.0   \n",
       "2011                0.0                0.0                0.0   \n",
       "2012                0.0                0.0                0.0   \n",
       "2013                0.0                0.0                0.0   \n",
       "2014                0.0                0.0                0.0   \n",
       "\n",
       "      53 Fantasy Points  54 Fantasy Points  55 Fantasy Points  \n",
       "0                   0.0                0.0                0.0  \n",
       "1                   0.0                0.0                0.0  \n",
       "2                   0.0                0.0                0.0  \n",
       "3                   0.0                0.0                0.0  \n",
       "4                   0.0                0.0                0.0  \n",
       "...                 ...                ...                ...  \n",
       "2010                0.0                0.0                0.0  \n",
       "2011                0.0                0.0                0.0  \n",
       "2012                0.0                0.0                0.0  \n",
       "2013                0.0                0.0                0.0  \n",
       "2014                0.0                0.0                0.0  \n",
       "\n",
       "[2015 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# first column: -5 points scored\n",
    "# last column: 55 points scored\n",
    "y = np.zeros(shape=(points_rounded.shape[0], 60))\n",
    "\n",
    "for i in range(y.shape[0]):\n",
    "    y[i, points_rounded.iloc[i] + 4] = 1\n",
    "\n",
    "y = pd.DataFrame(\n",
    "    y,\n",
    "    columns=[f\"{i - 4} Fantasy Points\" for i in range(y.shape[1])]\n",
    ")\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2020 Week 1\n",
       "1        2020 Week 1\n",
       "2        2020 Week 1\n",
       "3        2020 Week 1\n",
       "4        2020 Week 1\n",
       "            ...     \n",
       "2010    2023 Week 17\n",
       "2011    2023 Week 17\n",
       "2012    2023 Week 17\n",
       "2013    2023 Week 17\n",
       "2014    2023 Week 17\n",
       "Length: 2015, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = df_mod['Season'].astype(str) + ' Week ' + df_mod['Week'].astype(str)\n",
    "\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold ID</th>\n",
       "      <th>Season Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022 Week 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2021 Week 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2021 Week 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2023 Week 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2021 Week 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2</td>\n",
       "      <td>2020 Week 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2</td>\n",
       "      <td>2023 Week 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2</td>\n",
       "      <td>2021 Week 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2</td>\n",
       "      <td>2022 Week 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2</td>\n",
       "      <td>2022 Week 12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fold ID   Season Week\n",
       "0         0   2022 Week 3\n",
       "1         0   2021 Week 7\n",
       "2         0   2021 Week 1\n",
       "3         0  2023 Week 16\n",
       "4         0  2021 Week 11\n",
       "..      ...           ...\n",
       "62        2  2020 Week 15\n",
       "63        2  2023 Week 14\n",
       "64        2   2021 Week 5\n",
       "65        2   2022 Week 6\n",
       "66        2  2022 Week 12\n",
       "\n",
       "[67 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds = pd.read_parquet('../../../data/model_data/folds.parquet')\n",
    "\n",
    "df_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1105, 6) (216, 6) (694, 6) (1105, 60) (216, 60) (694, 60) (694, 18)\n",
      "(1156, 6) (212, 6) (647, 6) (1156, 60) (212, 60) (647, 60) (647, 18)\n",
      "(1137, 6) (204, 6) (674, 6) (1137, 60) (204, 60) (674, 60) (674, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# splitter = GroupKFold(n_splits=3)\n",
    "\n",
    "cv_data = []\n",
    "# for is_indexes, oos_indexes in splitter.split(X=X, y=y, groups=groups):\n",
    "for fold in df_folds['Fold ID'].unique():\n",
    "    oos_season_week = df_folds.loc[df_folds['Fold ID'] == fold, 'Season Week']\n",
    "    is_indexes = df_mod.loc[~groups.isin(oos_season_week), :].index\n",
    "    oos_indexes = df_mod.loc[groups.isin(oos_season_week), :].index\n",
    "    # split\n",
    "    X_is = X.iloc[is_indexes]\n",
    "    X_oos = X.iloc[oos_indexes]\n",
    "\n",
    "    y_is = y.iloc[is_indexes]\n",
    "    y_oos = y.iloc[oos_indexes]\n",
    "\n",
    "    groups_is = groups.iloc[is_indexes]\n",
    "    df_mod_oos = df_mod.iloc[oos_indexes]\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=22)\n",
    "    for train_indexes, val_indexes in gss.split(X=X_is, y=y_is, groups=groups_is):\n",
    "            X_train = X_is.iloc[train_indexes]\n",
    "            X_val = X_is.iloc[val_indexes]\n",
    "\n",
    "            y_train = y_is.iloc[train_indexes]\n",
    "            y_val = y_is.iloc[val_indexes]\n",
    "\n",
    "    # impute\n",
    "    scaler = StandardScaler()\n",
    "    imputer = IterativeImputer(initial_strategy='median', max_iter=100)\n",
    "\n",
    "    X_train_fill_na = imputer.fit_transform(scaler.fit_transform(X_train))\n",
    "    X_train[X_train.columns] = scaler.inverse_transform(X_train_fill_na).copy()\n",
    "    X_train['Prop Bets Projection'] = (\n",
    "        X_train['Adjusted Receiving Yards Projection']*0.1 + \n",
    "        X_train['Adjusted Receptions Projection']*0.5 + \n",
    "        X_train['Anytime Touchdown Probability']*6\n",
    "    )\n",
    "\n",
    "    scaler2 = MinMaxScaler(clip=True)  # maybe normalize and clip instead of standardize?\n",
    "    scaler3 = StandardScaler()\n",
    "    X_train[X_train.columns] = scaler3.fit_transform(scaler2.fit_transform(X_train)).copy()\n",
    "\n",
    "    X_val_fill_na = imputer.transform(scaler.transform(X_val))\n",
    "    X_val[X_val.columns] = scaler.inverse_transform(X_val_fill_na).copy()\n",
    "    X_val['Prop Bets Projection'] = (\n",
    "        X_val['Adjusted Receiving Yards Projection']*0.1 + \n",
    "        X_val['Adjusted Receptions Projection']*0.5 + \n",
    "        X_val['Anytime Touchdown Probability']*6\n",
    "    )\n",
    "\n",
    "    X_val[X_val.columns] = scaler3.transform(scaler2.transform(X_val)).copy()\n",
    "\n",
    "    X_oos_fill_na = imputer.transform(scaler.transform(X_oos))\n",
    "    X_oos[X_oos.columns] = scaler.inverse_transform(X_oos_fill_na).copy()\n",
    "    X_oos['Prop Bets Projection'] = (\n",
    "        X_oos['Adjusted Receiving Yards Projection']*0.1 + \n",
    "        X_oos['Adjusted Receptions Projection']*0.5 + \n",
    "        X_oos['Anytime Touchdown Probability']*6\n",
    "    )\n",
    "\n",
    "    X_oos[X_oos.columns] = scaler3.transform(scaler2.transform(X_oos)).copy()\n",
    "\n",
    "    X_train.drop(columns=['Adjusted Receiving Yards Projection', 'Adjusted Receptions Projection', 'Anytime Touchdown Probability'], inplace=True)\n",
    "    X_val.drop(columns=['Adjusted Receiving Yards Projection', 'Adjusted Receptions Projection', 'Anytime Touchdown Probability'], inplace=True)\n",
    "    X_oos.drop(columns=['Adjusted Receiving Yards Projection', 'Adjusted Receptions Projection', 'Anytime Touchdown Probability'], inplace=True)\n",
    "\n",
    "    cv_data.append((X_train, X_val, X_oos, y_train, y_val, y_oos, df_mod_oos))\n",
    "\n",
    "for X_train, X_val, X_oos, y_train, y_val, y_oos, df_mod_oos in cv_data:\n",
    "    print(X_train.shape, X_val.shape, X_oos.shape, y_train.shape, y_val.shape, y_oos.shape, df_mod_oos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 8)]               0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 128)               1152      \n",
      "                                                                 \n",
      " hidden_2 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " outputs (Dense)             (None, 60)                1980      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,260\n",
      "Trainable params: 7,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "from tensorflow.config.experimental import enable_op_determinism\n",
    "from tensorflow.random import set_seed\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "def build_and_compile_model(input_shape: tuple, output_shape: int, hidden_layer_neurons: list, l1s: list, l2s: list, learning_rate: float):\n",
    "    enable_op_determinism()\n",
    "    set_seed(22)\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='input')\n",
    "\n",
    "    h = inputs\n",
    "    for i, neurons in enumerate(hidden_layer_neurons):\n",
    "        h = Dense(\n",
    "            neurons, \n",
    "            activation='relu', \n",
    "            kernel_initializer=GlorotNormal(seed=22), \n",
    "            kernel_regularizer=L1L2(l1=l1s[i], l2=l2s[i]), \n",
    "            name=f\"hidden_{i+1}\"\n",
    "        )(h)\n",
    "\n",
    "    outputs = Dense(output_shape, activation='softmax', kernel_initializer=GlorotNormal(seed=22), name='outputs')(h)\n",
    "\n",
    "    mod = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    mod.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return mod\n",
    "\n",
    "mod = build_and_compile_model(X.shape[1:], y.shape[1], [128, 32], [0, 0.01], [0, 0.01], learning_rate=0.001)\n",
    "\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhugh\\anaconda3\\envs\\clean2\\lib\\site-packages\\optuna\\_experimental.py:30: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2024-10-22 14:16:23,019] A new study created in memory with name: no-name-ac218952-4877-47fb-9c56-7b1935b1f513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fb2ada675f4de3aa77c3ff3c7e25de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:16:46,566] Trial 0 finished with value: 2.9044927348644816 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.022040451663772567, 'hidden_layer_1_l2': 0.08119509205386867, 'learning_rate': 0.01094741868844976, 'batch_size': 2048}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:16:55,904] Trial 1 finished with value: 3.012893608680219 and parameters: {'n_hidden_layers': 3, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.058428964315689495, 'hidden_layer_1_l2': 0.07026355188654519, 'hidden_layer_2_neurons': 128, 'hidden_layer_2_l1': 0.08749277452777171, 'hidden_layer_2_l2': 0.07449320773686179, 'hidden_layer_3_neurons': 256, 'hidden_layer_3_l1': 0.009588460991804116, 'hidden_layer_3_l2': 0.004521011190769353, 'learning_rate': 0.07163458468004151, 'batch_size': 1024}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:17:08,645] Trial 2 finished with value: 2.933874851244037 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 64, 'hidden_layer_1_l1': 0.09115369973853776, 'hidden_layer_1_l2': 0.008625567976924198, 'hidden_layer_2_neurons': 2048, 'hidden_layer_2_l1': 0.0005512411903590642, 'hidden_layer_2_l2': 0.03576431697413079, 'learning_rate': 0.09578334240413691, 'batch_size': 1024}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:17:19,125] Trial 3 finished with value: 2.9153290785499206 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 1024, 'hidden_layer_1_l1': 0.08620878124622662, 'hidden_layer_1_l2': 0.06351961895274903, 'learning_rate': 0.07275008619568284, 'batch_size': 4096}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:17:27,525] Trial 4 finished with value: 2.938879874939286 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 1024, 'hidden_layer_1_l1': 0.02024276263846314, 'hidden_layer_1_l2': 0.08721894394962987, 'learning_rate': 0.06424371980220045, 'batch_size': 1024}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:17:40,963] Trial 5 finished with value: 2.905676171802568 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 32, 'hidden_layer_1_l1': 0.05653980354893057, 'hidden_layer_1_l2': 0.00345150171194456, 'learning_rate': 0.0885147769978044, 'batch_size': 16384}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:17:52,974] Trial 6 finished with value: 2.995890706171123 and parameters: {'n_hidden_layers': 3, 'hidden_layer_1_neurons': 256, 'hidden_layer_1_l1': 0.07564442330254927, 'hidden_layer_1_l2': 0.045177882434907994, 'hidden_layer_2_neurons': 64, 'hidden_layer_2_l1': 0.08362472880832089, 'hidden_layer_2_l2': 0.05103829122196473, 'hidden_layer_3_neurons': 128, 'hidden_layer_3_l1': 0.06951388599043494, 'hidden_layer_3_l2': 0.0051964104142108065, 'learning_rate': 0.09447015559422758, 'batch_size': 2048}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:18:04,017] Trial 7 finished with value: 3.0011012060813096 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 32, 'hidden_layer_1_l1': 0.08592805490779229, 'hidden_layer_1_l2': 0.0407904418229531, 'hidden_layer_2_neurons': 64, 'hidden_layer_2_l1': 0.025309932195697216, 'hidden_layer_2_l2': 0.03508986025005804, 'learning_rate': 0.06905762708317247, 'batch_size': 1024}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:18:14,726] Trial 8 finished with value: 2.939598231077089 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 32, 'hidden_layer_1_l1': 0.004671699281628905, 'hidden_layer_1_l2': 0.056923526796298245, 'learning_rate': 0.09827798409489276, 'batch_size': 1024}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:18:25,753] Trial 9 finished with value: 3.003712645966128 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 256, 'hidden_layer_1_l1': 0.0004424057058423636, 'hidden_layer_1_l2': 0.06813144847420316, 'hidden_layer_2_neurons': 64, 'hidden_layer_2_l1': 0.08249019408071431, 'hidden_layer_2_l2': 0.04359087388902699, 'learning_rate': 0.05221946484311475, 'batch_size': 1024}. Best is trial 0 with value: 2.9044927348644816.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:18:41,066] Trial 10 finished with value: 2.903453485534496 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.020822305720339832, 'hidden_layer_1_l2': 0.09105201059054474, 'learning_rate': 0.03633285507832701, 'batch_size': 2048}. Best is trial 10 with value: 2.903453485534496.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:18:57,739] Trial 11 finished with value: 2.907413106452506 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 1024, 'hidden_layer_1_l1': 0.02212847698075585, 'hidden_layer_1_l2': 0.0710101001486778, 'learning_rate': 0.01576495928001541, 'batch_size': 2048}. Best is trial 10 with value: 2.903453485534496.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:19:13,220] Trial 12 finished with value: 2.9008245362943574 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.009972878789203367, 'hidden_layer_1_l2': 0.09795208341638789, 'learning_rate': 0.01762343174680706, 'batch_size': 8192}. Best is trial 12 with value: 2.9008245362943574.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:19:28,007] Trial 13 finished with value: 2.913153296530944 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.02379020894960765, 'hidden_layer_1_l2': 0.09660691266249859, 'learning_rate': 0.04845148095888093, 'batch_size': 8192}. Best is trial 12 with value: 2.9008245362943574.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:19:41,803] Trial 14 finished with value: 3.0007103006738913 and parameters: {'n_hidden_layers': 3, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.0011362657820468622, 'hidden_layer_1_l2': 0.0807641316297729, 'hidden_layer_2_neurons': 512, 'hidden_layer_2_l1': 0.04490480620139892, 'hidden_layer_2_l2': 0.09940529653696407, 'hidden_layer_3_neurons': 512, 'hidden_layer_3_l1': 0.09590393621471581, 'hidden_layer_3_l2': 0.09350909781333765, 'learning_rate': 0.02333023751468171, 'batch_size': 16384}. Best is trial 12 with value: 2.9008245362943574.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:19:54,799] Trial 15 finished with value: 2.90402893627878 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.0352728673512092, 'hidden_layer_1_l2': 0.06035408080311322, 'learning_rate': 0.06578545315242046, 'batch_size': 2048}. Best is trial 12 with value: 2.9008245362943574.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:20:07,913] Trial 16 finished with value: 2.9361415172320866 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 2048, 'hidden_layer_1_l1': 0.0017160411761518342, 'hidden_layer_1_l2': 0.09901166479356358, 'learning_rate': 0.05211905265016894, 'batch_size': 2048}. Best is trial 12 with value: 2.9008245362943574.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:20:26,553] Trial 17 finished with value: 2.9997163326922003 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 32, 'hidden_layer_1_l1': 0.043062927799839695, 'hidden_layer_1_l2': 0.09536025359098428, 'hidden_layer_2_neurons': 256, 'hidden_layer_2_l1': 0.04935473615279186, 'hidden_layer_2_l2': 0.008411919297923787, 'learning_rate': 0.02176214482871601, 'batch_size': 32768}. Best is trial 12 with value: 2.9008245362943574.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:20:47,343] Trial 18 finished with value: 2.90487366032515 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 32, 'hidden_layer_1_l1': 0.026345715371129053, 'hidden_layer_1_l2': 0.07995828746316233, 'learning_rate': 0.026139815008333737, 'batch_size': 8192}. Best is trial 12 with value: 2.9008245362943574.\n",
      "(2015, 60) (2015, 60)\n",
      "[I 2024-10-22 14:21:04,742] Trial 19 finished with value: 2.906163754830667 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 64, 'hidden_layer_1_l1': 0.04984064488240802, 'hidden_layer_1_l2': 0.08370316060890715, 'learning_rate': 0.0441879449959942, 'batch_size': 16384}. Best is trial 12 with value: 2.9008245362943574.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_hidden_layers': 1,\n",
       " 'hidden_layer_1_neurons': 128,\n",
       " 'hidden_layer_1_l1': 0.009972878789203367,\n",
       " 'hidden_layer_1_l2': 0.09795208341638789,\n",
       " 'learning_rate': 0.01762343174680706,\n",
       " 'batch_size': 8192}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import optuna\n",
    "\n",
    "def objective(trial, cv_data=cv_data):\n",
    "# model tuning\n",
    "    n_hidden_layers = trial.suggest_int(f\"n_hidden_layers\", 1, 3)\n",
    "\n",
    "    hidden_layer_neurons = []\n",
    "    l1s = []\n",
    "    l2s = []\n",
    "    for i in range(n_hidden_layers):\n",
    "        hidden_layer_neurons.append(trial.suggest_categorical(f\"hidden_layer_{i+1}_neurons\", [2**n for n in range(5, 12)]))  # change to (3, 12)\n",
    "        # hidden_layer_neurons.append(trial.suggest_categorical(f\"hidden_layer_{i+1}_neurons\", [2**n for n in range(4, 9)]))  # bump this up\n",
    "        l1s.append(trial.suggest_float(f\"hidden_layer_{i+1}_l1\", 0.0, 0.1))  # change to (0.0, 0.05)\n",
    "        l2s.append(trial.suggest_float(f\"hidden_layer_{i+1}_l2\", 0.0, 0.1))  # change to (0.0, 0.20)\n",
    "\n",
    "    learning_rate = trial.suggest_float(f\"learning_rate\", 0.01, 0.10)\n",
    "    batch_size = trial.suggest_categorical(f\"batch_size\", [2**n for n in range(10, 16)])\n",
    "    # batch_size = trial.suggest_categorical(f\"batch_size\", [2**n for n in range(4, 12)])  # bump this up\n",
    "\n",
    "    # cross validation\n",
    "    y_oos_list = []\n",
    "    y_pred_list = []\n",
    "    for X_train, X_val, X_oos, y_train, y_val, y_oos, df_mod_oos in cv_data:\n",
    "        # make sure to build mod in loop to prevent history\n",
    "        mod = build_and_compile_model(X_train.shape[1:], y_train.shape[1], hidden_layer_neurons, l1s, l2s, learning_rate)\n",
    "\n",
    "        mod.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=500,\n",
    "            callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        y_oos_list.append(y_oos)\n",
    "        y_pred_list.append(mod.predict(X_oos, verbose=0))\n",
    "\n",
    "\n",
    "    y_oos_concat = np.vstack(y_oos_list)\n",
    "    y_pred_concat = np.vstack(y_pred_list)\n",
    "\n",
    "    print(y_oos_concat.shape, y_pred_concat.shape)\n",
    "\n",
    "    return log_loss(y_oos_concat, y_pred_concat)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=22, n_startup_trials=10, multivariate=True, warn_independent_sampling=False))\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9008245362943574"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.0267 - accuracy: 0.0090 - val_loss: 5.3242 - val_accuracy: 0.0741\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.3439 - accuracy: 0.0833 - val_loss: 4.7901 - val_accuracy: 0.0880\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.8246 - accuracy: 0.0824 - val_loss: 4.3685 - val_accuracy: 0.0880\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4.4106 - accuracy: 0.0905 - val_loss: 4.0347 - val_accuracy: 0.0926\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4.0799 - accuracy: 0.0977 - val_loss: 3.7657 - val_accuracy: 0.0833\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.8116 - accuracy: 0.1005 - val_loss: 3.5502 - val_accuracy: 0.0926\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.5961 - accuracy: 0.1122 - val_loss: 3.3842 - val_accuracy: 0.0926\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4311 - accuracy: 0.1176 - val_loss: 3.2659 - val_accuracy: 0.1111\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.3146 - accuracy: 0.1348 - val_loss: 3.1894 - val_accuracy: 0.1204\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.2428 - accuracy: 0.1303 - val_loss: 3.1431 - val_accuracy: 0.1343\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.2028 - accuracy: 0.1240 - val_loss: 3.1118 - val_accuracy: 0.1111\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.1784 - accuracy: 0.1140 - val_loss: 3.0888 - val_accuracy: 0.1296\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.1610 - accuracy: 0.1077 - val_loss: 3.0708 - val_accuracy: 0.1204\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1457 - accuracy: 0.1023 - val_loss: 3.0548 - val_accuracy: 0.1157\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.1322 - accuracy: 0.1023 - val_loss: 3.0423 - val_accuracy: 0.1296\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.1224 - accuracy: 0.1059 - val_loss: 3.0290 - val_accuracy: 0.1296\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.1122 - accuracy: 0.1158 - val_loss: 3.0164 - val_accuracy: 0.1250\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.1024 - accuracy: 0.1104 - val_loss: 3.0077 - val_accuracy: 0.1157\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.0946 - accuracy: 0.1023 - val_loss: 3.0002 - val_accuracy: 0.1111\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0859 - accuracy: 0.1167 - val_loss: 2.9933 - val_accuracy: 0.0880\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0753 - accuracy: 0.1267 - val_loss: 2.9864 - val_accuracy: 0.0972\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0629 - accuracy: 0.1312 - val_loss: 2.9782 - val_accuracy: 0.0926\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.0483 - accuracy: 0.1231 - val_loss: 2.9697 - val_accuracy: 0.0972\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0337 - accuracy: 0.1140 - val_loss: 2.9620 - val_accuracy: 0.1065\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.0212 - accuracy: 0.1167 - val_loss: 2.9524 - val_accuracy: 0.1250\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.0084 - accuracy: 0.1312 - val_loss: 2.9438 - val_accuracy: 0.1157\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9978 - accuracy: 0.1149 - val_loss: 2.9387 - val_accuracy: 0.1204\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9913 - accuracy: 0.1158 - val_loss: 2.9356 - val_accuracy: 0.1204\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9872 - accuracy: 0.1140 - val_loss: 2.9322 - val_accuracy: 0.1204\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9826 - accuracy: 0.1140 - val_loss: 2.9288 - val_accuracy: 0.1157\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2.9775 - accuracy: 0.1158 - val_loss: 2.9244 - val_accuracy: 0.1250\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9711 - accuracy: 0.1213 - val_loss: 2.9202 - val_accuracy: 0.1296\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9655 - accuracy: 0.1213 - val_loss: 2.9170 - val_accuracy: 0.1389\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9616 - accuracy: 0.1204 - val_loss: 2.9140 - val_accuracy: 0.1343\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9582 - accuracy: 0.1276 - val_loss: 2.9102 - val_accuracy: 0.1343\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9545 - accuracy: 0.1276 - val_loss: 2.9058 - val_accuracy: 0.1250\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.9505 - accuracy: 0.1285 - val_loss: 2.9015 - val_accuracy: 0.1204\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9468 - accuracy: 0.1294 - val_loss: 2.8976 - val_accuracy: 0.1296\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9437 - accuracy: 0.1294 - val_loss: 2.8930 - val_accuracy: 0.1296\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9408 - accuracy: 0.1240 - val_loss: 2.8881 - val_accuracy: 0.1343\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9380 - accuracy: 0.1222 - val_loss: 2.8836 - val_accuracy: 0.1389\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9355 - accuracy: 0.1231 - val_loss: 2.8802 - val_accuracy: 0.1389\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9334 - accuracy: 0.1195 - val_loss: 2.8782 - val_accuracy: 0.1296\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9322 - accuracy: 0.1176 - val_loss: 2.8765 - val_accuracy: 0.1389\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9303 - accuracy: 0.1195 - val_loss: 2.8762 - val_accuracy: 0.1389\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9283 - accuracy: 0.1213 - val_loss: 2.8770 - val_accuracy: 0.1343\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9263 - accuracy: 0.1240 - val_loss: 2.8783 - val_accuracy: 0.1296\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9243 - accuracy: 0.1240 - val_loss: 2.8800 - val_accuracy: 0.1343\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9232 - accuracy: 0.1249 - val_loss: 2.8815 - val_accuracy: 0.1343\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.9223 - accuracy: 0.1267 - val_loss: 2.8816 - val_accuracy: 0.1343\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9209 - accuracy: 0.1249 - val_loss: 2.8808 - val_accuracy: 0.1343\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.9196 - accuracy: 0.1249 - val_loss: 2.8800 - val_accuracy: 0.1296\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9187 - accuracy: 0.1240 - val_loss: 2.8791 - val_accuracy: 0.1296\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9178 - accuracy: 0.1240 - val_loss: 2.8786 - val_accuracy: 0.1296\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9171 - accuracy: 0.1231 - val_loss: 2.8777 - val_accuracy: 0.1389\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.0313 - accuracy: 0.0104 - val_loss: 5.3266 - val_accuracy: 0.0755\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 5.3404 - accuracy: 0.0753 - val_loss: 4.8086 - val_accuracy: 0.0943\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.8133 - accuracy: 0.0830 - val_loss: 4.3914 - val_accuracy: 0.1179\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 4.3932 - accuracy: 0.1047 - val_loss: 4.0533 - val_accuracy: 0.1132\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 4.0568 - accuracy: 0.1185 - val_loss: 3.7758 - val_accuracy: 0.1226\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 3.7842 - accuracy: 0.1228 - val_loss: 3.5503 - val_accuracy: 0.1179\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.5664 - accuracy: 0.1324 - val_loss: 3.3741 - val_accuracy: 0.1179\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 3.3997 - accuracy: 0.1393 - val_loss: 3.2445 - val_accuracy: 0.1132\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.2811 - accuracy: 0.1427 - val_loss: 3.1571 - val_accuracy: 0.1179\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.2059 - accuracy: 0.1289 - val_loss: 3.1035 - val_accuracy: 0.1085\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.1639 - accuracy: 0.1280 - val_loss: 3.0683 - val_accuracy: 0.0896\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.1401 - accuracy: 0.1315 - val_loss: 3.0432 - val_accuracy: 0.1179\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.1236 - accuracy: 0.1263 - val_loss: 3.0241 - val_accuracy: 0.1179\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.1080 - accuracy: 0.1211 - val_loss: 3.0108 - val_accuracy: 0.1085\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 3.0931 - accuracy: 0.1151 - val_loss: 3.0073 - val_accuracy: 0.1226\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0826 - accuracy: 0.1185 - val_loss: 3.0110 - val_accuracy: 0.0991\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0748 - accuracy: 0.1246 - val_loss: 3.0180 - val_accuracy: 0.1085\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.0677 - accuracy: 0.1289 - val_loss: 3.0244 - val_accuracy: 0.1038\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0612 - accuracy: 0.1436 - val_loss: 3.0248 - val_accuracy: 0.0896\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 3.0525 - accuracy: 0.1453 - val_loss: 3.0176 - val_accuracy: 0.0991\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.0407 - accuracy: 0.1410 - val_loss: 3.0041 - val_accuracy: 0.1274\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.0266 - accuracy: 0.1341 - val_loss: 2.9858 - val_accuracy: 0.1179\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 3.0101 - accuracy: 0.1341 - val_loss: 2.9664 - val_accuracy: 0.1132\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.9938 - accuracy: 0.1358 - val_loss: 2.9479 - val_accuracy: 0.1274\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9786 - accuracy: 0.1410 - val_loss: 2.9305 - val_accuracy: 0.1179\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.9637 - accuracy: 0.1358 - val_loss: 2.9162 - val_accuracy: 0.1274\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9516 - accuracy: 0.1384 - val_loss: 2.9067 - val_accuracy: 0.1179\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2.9440 - accuracy: 0.1332 - val_loss: 2.8992 - val_accuracy: 0.1132\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.9384 - accuracy: 0.1384 - val_loss: 2.8927 - val_accuracy: 0.1132\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9340 - accuracy: 0.1375 - val_loss: 2.8862 - val_accuracy: 0.1132\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9295 - accuracy: 0.1393 - val_loss: 2.8791 - val_accuracy: 0.1274\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.9243 - accuracy: 0.1410 - val_loss: 2.8727 - val_accuracy: 0.1179\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9194 - accuracy: 0.1436 - val_loss: 2.8678 - val_accuracy: 0.1321\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9155 - accuracy: 0.1436 - val_loss: 2.8644 - val_accuracy: 0.1321\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9122 - accuracy: 0.1436 - val_loss: 2.8625 - val_accuracy: 0.1226\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9090 - accuracy: 0.1445 - val_loss: 2.8615 - val_accuracy: 0.1226\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.9061 - accuracy: 0.1471 - val_loss: 2.8601 - val_accuracy: 0.1179\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9024 - accuracy: 0.1462 - val_loss: 2.8577 - val_accuracy: 0.1179\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8983 - accuracy: 0.1471 - val_loss: 2.8555 - val_accuracy: 0.1226\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8948 - accuracy: 0.1505 - val_loss: 2.8534 - val_accuracy: 0.1226\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2.8926 - accuracy: 0.1488 - val_loss: 2.8514 - val_accuracy: 0.1226\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8909 - accuracy: 0.1453 - val_loss: 2.8488 - val_accuracy: 0.1321\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8886 - accuracy: 0.1410 - val_loss: 2.8461 - val_accuracy: 0.1368\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8855 - accuracy: 0.1375 - val_loss: 2.8453 - val_accuracy: 0.1368\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.8834 - accuracy: 0.1393 - val_loss: 2.8452 - val_accuracy: 0.1274\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8819 - accuracy: 0.1436 - val_loss: 2.8451 - val_accuracy: 0.1226\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8801 - accuracy: 0.1462 - val_loss: 2.8455 - val_accuracy: 0.1321\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8786 - accuracy: 0.1445 - val_loss: 2.8455 - val_accuracy: 0.1368\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 2.8769 - accuracy: 0.1453 - val_loss: 2.8459 - val_accuracy: 0.1368\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8762 - accuracy: 0.1471 - val_loss: 2.8457 - val_accuracy: 0.1132\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8753 - accuracy: 0.1471 - val_loss: 2.8444 - val_accuracy: 0.1179\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8732 - accuracy: 0.1453 - val_loss: 2.8433 - val_accuracy: 0.1226\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8712 - accuracy: 0.1462 - val_loss: 2.8427 - val_accuracy: 0.1274\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8702 - accuracy: 0.1453 - val_loss: 2.8425 - val_accuracy: 0.1274\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.8693 - accuracy: 0.1471 - val_loss: 2.8427 - val_accuracy: 0.1226\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 2.8683 - accuracy: 0.1488 - val_loss: 2.8431 - val_accuracy: 0.1132\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8673 - accuracy: 0.1488 - val_loss: 2.8434 - val_accuracy: 0.1085\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.8662 - accuracy: 0.1462 - val_loss: 2.8427 - val_accuracy: 0.1085\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8652 - accuracy: 0.1453 - val_loss: 2.8418 - val_accuracy: 0.1179\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8644 - accuracy: 0.1479 - val_loss: 2.8410 - val_accuracy: 0.1226\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.8637 - accuracy: 0.1471 - val_loss: 2.8396 - val_accuracy: 0.1226\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8627 - accuracy: 0.1462 - val_loss: 2.8386 - val_accuracy: 0.1226\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8622 - accuracy: 0.1462 - val_loss: 2.8381 - val_accuracy: 0.1179\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8613 - accuracy: 0.1462 - val_loss: 2.8385 - val_accuracy: 0.1226\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8606 - accuracy: 0.1479 - val_loss: 2.8385 - val_accuracy: 0.1085\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8596 - accuracy: 0.1497 - val_loss: 2.8389 - val_accuracy: 0.1179\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8592 - accuracy: 0.1522 - val_loss: 2.8391 - val_accuracy: 0.1132\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8581 - accuracy: 0.1531 - val_loss: 2.8394 - val_accuracy: 0.1085\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.8572 - accuracy: 0.1540 - val_loss: 2.8391 - val_accuracy: 0.1085\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8566 - accuracy: 0.1557 - val_loss: 2.8376 - val_accuracy: 0.1085\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.8559 - accuracy: 0.1540 - val_loss: 2.8370 - val_accuracy: 0.1132\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8552 - accuracy: 0.1522 - val_loss: 2.8369 - val_accuracy: 0.1179\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8545 - accuracy: 0.1522 - val_loss: 2.8373 - val_accuracy: 0.1226\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8541 - accuracy: 0.1522 - val_loss: 2.8372 - val_accuracy: 0.1179\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8536 - accuracy: 0.1531 - val_loss: 2.8368 - val_accuracy: 0.1226\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8528 - accuracy: 0.1548 - val_loss: 2.8369 - val_accuracy: 0.1226\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8522 - accuracy: 0.1522 - val_loss: 2.8364 - val_accuracy: 0.1226\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 2.8516 - accuracy: 0.1540 - val_loss: 2.8355 - val_accuracy: 0.1226\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8509 - accuracy: 0.1522 - val_loss: 2.8350 - val_accuracy: 0.1179\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8502 - accuracy: 0.1531 - val_loss: 2.8360 - val_accuracy: 0.1132\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8501 - accuracy: 0.1531 - val_loss: 2.8377 - val_accuracy: 0.1132\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.8498 - accuracy: 0.1540 - val_loss: 2.8373 - val_accuracy: 0.1179\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2.8483 - accuracy: 0.1522 - val_loss: 2.8369 - val_accuracy: 0.1132\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8476 - accuracy: 0.1488 - val_loss: 2.8376 - val_accuracy: 0.1179\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8470 - accuracy: 0.1522 - val_loss: 2.8379 - val_accuracy: 0.1179\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8473 - accuracy: 0.1557 - val_loss: 2.8369 - val_accuracy: 0.1226\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8466 - accuracy: 0.1540 - val_loss: 2.8368 - val_accuracy: 0.1226\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8456 - accuracy: 0.1540 - val_loss: 2.8374 - val_accuracy: 0.1179\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.8452 - accuracy: 0.1522 - val_loss: 2.8389 - val_accuracy: 0.1179\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 914ms/step - loss: 6.0259 - accuracy: 0.0026 - val_loss: 5.3354 - val_accuracy: 0.0735\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.3322 - accuracy: 0.0800 - val_loss: 4.8159 - val_accuracy: 0.0735\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4.8019 - accuracy: 0.0827 - val_loss: 4.4022 - val_accuracy: 0.0882\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4.3782 - accuracy: 0.0941 - val_loss: 4.0738 - val_accuracy: 0.0833\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.0403 - accuracy: 0.1011 - val_loss: 3.8101 - val_accuracy: 0.1029\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.7661 - accuracy: 0.1135 - val_loss: 3.6005 - val_accuracy: 0.1029\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.5465 - accuracy: 0.1214 - val_loss: 3.4420 - val_accuracy: 0.1029\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.3767 - accuracy: 0.1240 - val_loss: 3.3357 - val_accuracy: 0.0980\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.2571 - accuracy: 0.1240 - val_loss: 3.2743 - val_accuracy: 0.0833\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.1815 - accuracy: 0.1196 - val_loss: 3.2450 - val_accuracy: 0.0980\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.1385 - accuracy: 0.1223 - val_loss: 3.2304 - val_accuracy: 0.0980\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.1132 - accuracy: 0.1135 - val_loss: 3.2197 - val_accuracy: 0.0686\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0976 - accuracy: 0.1170 - val_loss: 3.2053 - val_accuracy: 0.0637\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.0851 - accuracy: 0.1073 - val_loss: 3.1834 - val_accuracy: 0.0637\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0741 - accuracy: 0.1082 - val_loss: 3.1558 - val_accuracy: 0.0735\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0627 - accuracy: 0.1108 - val_loss: 3.1286 - val_accuracy: 0.0980\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.0524 - accuracy: 0.1179 - val_loss: 3.1040 - val_accuracy: 0.1225\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0419 - accuracy: 0.1187 - val_loss: 3.0851 - val_accuracy: 0.1029\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.0322 - accuracy: 0.1161 - val_loss: 3.0710 - val_accuracy: 0.1176\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0210 - accuracy: 0.1231 - val_loss: 3.0588 - val_accuracy: 0.1078\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.0063 - accuracy: 0.1196 - val_loss: 3.0479 - val_accuracy: 0.1078\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 2.9897 - accuracy: 0.1152 - val_loss: 3.0389 - val_accuracy: 0.0882\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 2.9736 - accuracy: 0.1179 - val_loss: 3.0312 - val_accuracy: 0.0882\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.9594 - accuracy: 0.1179 - val_loss: 3.0232 - val_accuracy: 0.0931\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9463 - accuracy: 0.1152 - val_loss: 3.0156 - val_accuracy: 0.0833\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9354 - accuracy: 0.1143 - val_loss: 3.0080 - val_accuracy: 0.0931\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9262 - accuracy: 0.1196 - val_loss: 3.0015 - val_accuracy: 0.0980\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.9193 - accuracy: 0.1205 - val_loss: 2.9960 - val_accuracy: 0.0980\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9139 - accuracy: 0.1258 - val_loss: 2.9915 - val_accuracy: 0.1029\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9085 - accuracy: 0.1275 - val_loss: 2.9876 - val_accuracy: 0.0980\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9028 - accuracy: 0.1293 - val_loss: 2.9852 - val_accuracy: 0.1029\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8975 - accuracy: 0.1275 - val_loss: 2.9850 - val_accuracy: 0.0980\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8943 - accuracy: 0.1196 - val_loss: 2.9841 - val_accuracy: 0.0882\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8912 - accuracy: 0.1143 - val_loss: 2.9803 - val_accuracy: 0.0980\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8869 - accuracy: 0.1161 - val_loss: 2.9758 - val_accuracy: 0.1029\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8829 - accuracy: 0.1214 - val_loss: 2.9714 - val_accuracy: 0.1029\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8797 - accuracy: 0.1214 - val_loss: 2.9671 - val_accuracy: 0.0931\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8762 - accuracy: 0.1231 - val_loss: 2.9635 - val_accuracy: 0.0882\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8733 - accuracy: 0.1205 - val_loss: 2.9586 - val_accuracy: 0.0980\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8702 - accuracy: 0.1214 - val_loss: 2.9526 - val_accuracy: 0.0931\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8670 - accuracy: 0.1249 - val_loss: 2.9462 - val_accuracy: 0.0931\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8638 - accuracy: 0.1249 - val_loss: 2.9410 - val_accuracy: 0.0931\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8611 - accuracy: 0.1249 - val_loss: 2.9364 - val_accuracy: 0.1029\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8587 - accuracy: 0.1266 - val_loss: 2.9335 - val_accuracy: 0.1127\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8570 - accuracy: 0.1266 - val_loss: 2.9314 - val_accuracy: 0.1029\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8557 - accuracy: 0.1258 - val_loss: 2.9288 - val_accuracy: 0.1029\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8542 - accuracy: 0.1231 - val_loss: 2.9257 - val_accuracy: 0.0980\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8523 - accuracy: 0.1214 - val_loss: 2.9234 - val_accuracy: 0.0931\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.8509 - accuracy: 0.1223 - val_loss: 2.9221 - val_accuracy: 0.0931\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 2.8496 - accuracy: 0.1240 - val_loss: 2.9217 - val_accuracy: 0.0980\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8485 - accuracy: 0.1249 - val_loss: 2.9217 - val_accuracy: 0.0980\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8474 - accuracy: 0.1223 - val_loss: 2.9220 - val_accuracy: 0.1029\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 2.8469 - accuracy: 0.1249 - val_loss: 2.9219 - val_accuracy: 0.0980\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8460 - accuracy: 0.1249 - val_loss: 2.9214 - val_accuracy: 0.0980\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8445 - accuracy: 0.1223 - val_loss: 2.9213 - val_accuracy: 0.0980\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.8436 - accuracy: 0.1223 - val_loss: 2.9213 - val_accuracy: 0.0980\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8423 - accuracy: 0.1249 - val_loss: 2.9214 - val_accuracy: 0.1029\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8414 - accuracy: 0.1249 - val_loss: 2.9213 - val_accuracy: 0.1029\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8406 - accuracy: 0.1266 - val_loss: 2.9218 - val_accuracy: 0.0980\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8400 - accuracy: 0.1266 - val_loss: 2.9222 - val_accuracy: 0.0980\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.8392 - accuracy: 0.1266 - val_loss: 2.9212 - val_accuracy: 0.0980\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8379 - accuracy: 0.1275 - val_loss: 2.9197 - val_accuracy: 0.0980\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 2.8369 - accuracy: 0.1302 - val_loss: 2.9188 - val_accuracy: 0.0980\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8367 - accuracy: 0.1293 - val_loss: 2.9183 - val_accuracy: 0.0980\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 2.8360 - accuracy: 0.1275 - val_loss: 2.9172 - val_accuracy: 0.0980\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8347 - accuracy: 0.1258 - val_loss: 2.9165 - val_accuracy: 0.1029\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8337 - accuracy: 0.1284 - val_loss: 2.9167 - val_accuracy: 0.1029\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8334 - accuracy: 0.1249 - val_loss: 2.9163 - val_accuracy: 0.0980\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8331 - accuracy: 0.1266 - val_loss: 2.9156 - val_accuracy: 0.1029\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.8325 - accuracy: 0.1293 - val_loss: 2.9154 - val_accuracy: 0.1078\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8317 - accuracy: 0.1293 - val_loss: 2.9146 - val_accuracy: 0.1078\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8307 - accuracy: 0.1310 - val_loss: 2.9149 - val_accuracy: 0.1078\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8304 - accuracy: 0.1302 - val_loss: 2.9154 - val_accuracy: 0.1029\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8300 - accuracy: 0.1328 - val_loss: 2.9144 - val_accuracy: 0.1029\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8291 - accuracy: 0.1284 - val_loss: 2.9141 - val_accuracy: 0.1029\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8288 - accuracy: 0.1284 - val_loss: 2.9142 - val_accuracy: 0.1029\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8283 - accuracy: 0.1319 - val_loss: 2.9142 - val_accuracy: 0.1029\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8277 - accuracy: 0.1319 - val_loss: 2.9142 - val_accuracy: 0.0980\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8270 - accuracy: 0.1310 - val_loss: 2.9130 - val_accuracy: 0.1127\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.8261 - accuracy: 0.1310 - val_loss: 2.9125 - val_accuracy: 0.1127\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8253 - accuracy: 0.1293 - val_loss: 2.9135 - val_accuracy: 0.1078\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.8251 - accuracy: 0.1249 - val_loss: 2.9144 - val_accuracy: 0.1078\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8253 - accuracy: 0.1275 - val_loss: 2.9156 - val_accuracy: 0.1127\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8248 - accuracy: 0.1302 - val_loss: 2.9156 - val_accuracy: 0.1176\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8238 - accuracy: 0.1337 - val_loss: 2.9153 - val_accuracy: 0.1176\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8230 - accuracy: 0.1363 - val_loss: 2.9142 - val_accuracy: 0.1176\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.8224 - accuracy: 0.1363 - val_loss: 2.9137 - val_accuracy: 0.1176\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.8222 - accuracy: 0.1328 - val_loss: 2.9146 - val_accuracy: 0.1078\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.8223 - accuracy: 0.1302 - val_loss: 2.9156 - val_accuracy: 0.1078\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.8218 - accuracy: 0.1284 - val_loss: 2.9148 - val_accuracy: 0.1127\n",
      "OOS Log Loss: 2.9008245362943574\n"
     ]
    }
   ],
   "source": [
    "n_hidden_layers = study.best_params['n_hidden_layers']\n",
    "\n",
    "hidden_layer_neurons = []\n",
    "l1s = []\n",
    "l2s = []\n",
    "for i in range(n_hidden_layers):\n",
    "    hidden_layer_neurons.append(study.best_params[f\"hidden_layer_{i+1}_neurons\"])\n",
    "    l1s.append(study.best_params[f\"hidden_layer_{i+1}_l1\"])\n",
    "    l2s.append(study.best_params[f\"hidden_layer_{i+1}_l2\"])\n",
    "\n",
    "learning_rate = study.best_params[f\"learning_rate\"]\n",
    "batch_size = study.best_params[f\"batch_size\"]\n",
    "\n",
    "y_oos_list = []\n",
    "y_pred_list = []\n",
    "testing_data = []\n",
    "for X_train, X_val, X_oos, y_train, y_val, y_oos, df_mod_oos in cv_data:\n",
    "    # make sure to build mod in loop to prevent history\n",
    "    mod = build_and_compile_model(X_train.shape[1:], y_train.shape[1], hidden_layer_neurons, l1s, l2s, learning_rate)\n",
    "\n",
    "    mod.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=500,\n",
    "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    y_preds = mod.predict(X_oos, verbose=0)\n",
    "\n",
    "    df_preds = pd.DataFrame(\n",
    "        y_preds,\n",
    "        columns=y.columns\n",
    "    )\n",
    "\n",
    "    testing_data.append(pd.concat((df_mod_oos.reset_index(drop=True), df_preds), axis=1))\n",
    "\n",
    "\n",
    "    y_oos_list.append(y_oos)\n",
    "    y_pred_list.append(y_preds)\n",
    "\n",
    "df_test = pd.concat(testing_data, ignore_index=True)\n",
    "\n",
    "y_oos_concat = np.vstack(y_oos_list)\n",
    "y_pred_concat = np.vstack(y_pred_list)\n",
    "\n",
    "print(f\"OOS Log Loss: {log_loss(y_oos_concat, y_pred_concat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_distribution(row):\n",
    "    distribution = row[[f\"{i} Fantasy Points\" for i in range(-4, 56)]].astype('float64')/row[[f\"{i} Fantasy Points\" for i in range(-4, 56)]].astype('float64').sum()\n",
    "\n",
    "    return np.random.choice([i for i in range(-4, 56)], p=distribution, size=1_000)\n",
    "\n",
    "# get_random_distribution(df_test.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_test['Fantasy Points'] < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(22)\n",
    "\n",
    "row = df_test.iloc[1_000]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "random_distribution = get_random_distribution(row)\n",
    "\n",
    "ax = sns.kdeplot(random_distribution, fill=True, bw_adjust=1.0, color='blue', label='Projected Fantasy Points Distribution')\n",
    "\n",
    "current_limits = ax.get_ylim()\n",
    "\n",
    "plt.vlines(x=row['Fantasy Points'], ymin=0.0, ymax=1.0, colors=['red'], label='Actual Fantasy Points Scored')\n",
    "\n",
    "ax.set_ylim(current_limits)\n",
    "\n",
    "plt.title(f\"{row['Name'].title()} {row['Season']} Week {row['Week']} Projected Fantasy Points Distribution\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.358649306746217"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "root_mean_squared_error(\n",
    "    df_test['Fantasy Points'],\n",
    "    np.dot(np.array([i for i in range(-4, 56)]), df_test[[f\"{i} Fantasy Points\" for i in range(-4, 56)]].transpose()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.079136385052907"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(\n",
    "    df_test['Fantasy Points'],\n",
    "    np.dot(np.array([i for i in range(-4, 56)]), df_test[[f\"{i} Fantasy Points\" for i in range(-4, 56)]].transpose()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often do players end up in certain percentiles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles = [(df_test.iloc[i]['Fantasy Points'] > get_random_distribution(df_test.iloc[i])).mean() for i in range(df_test.shape[0])]\n",
    "\n",
    "len(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(\n",
    "    x=percentiles,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2015.000000\n",
       "mean        0.494429\n",
       "std         0.293356\n",
       "min         0.000000\n",
       "1%          0.000000\n",
       "5%          0.000000\n",
       "10%         0.076200\n",
       "25%         0.252000\n",
       "50%         0.504000\n",
       "75%         0.744000\n",
       "90%         0.892600\n",
       "95%         0.943000\n",
       "99%         0.989000\n",
       "max         0.998000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(percentiles).describe(percentiles=[0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
