{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Week</th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "      <th>Team</th>\n",
       "      <th>Fantasy Points</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Position Rank</th>\n",
       "      <th>Adjusted Passing Yards Projection</th>\n",
       "      <th>Adjusted Passing Touchdowns Projection</th>\n",
       "      <th>Adjusted Interceptions Projection</th>\n",
       "      <th>Adjusted Rushing Yards Projection</th>\n",
       "      <th>Adjusted Receiving Yards Projection</th>\n",
       "      <th>Adjusted Receptions Projection</th>\n",
       "      <th>Anytime Touchdown Probability</th>\n",
       "      <th>Location</th>\n",
       "      <th>Team Projected Score</th>\n",
       "      <th>Opponent Projected Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>patrick mahomes</td>\n",
       "      <td>QB</td>\n",
       "      <td>KC</td>\n",
       "      <td>20.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>306.500000</td>\n",
       "      <td>2.460615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.488656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.50</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>lamar jackson</td>\n",
       "      <td>QB</td>\n",
       "      <td>BAL</td>\n",
       "      <td>27.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>222.500000</td>\n",
       "      <td>1.528860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.25</td>\n",
       "      <td>20.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>dak prescott</td>\n",
       "      <td>QB</td>\n",
       "      <td>DAL</td>\n",
       "      <td>17.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>285.500000</td>\n",
       "      <td>2.495487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.517041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.230947</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>25.50</td>\n",
       "      <td>26.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>russell wilson</td>\n",
       "      <td>QB</td>\n",
       "      <td>SEA</td>\n",
       "      <td>31.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>259.500000</td>\n",
       "      <td>1.564394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>deshaun watson</td>\n",
       "      <td>QB</td>\n",
       "      <td>HOU</td>\n",
       "      <td>20.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>275.500000</td>\n",
       "      <td>1.574315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.512870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>31.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>taylor heinicke</td>\n",
       "      <td>QB</td>\n",
       "      <td>ATL</td>\n",
       "      <td>15.1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>28</td>\n",
       "      <td>203.494463</td>\n",
       "      <td>1.304627</td>\n",
       "      <td>0.510823</td>\n",
       "      <td>10.504409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>18.25</td>\n",
       "      <td>20.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>cj beathard</td>\n",
       "      <td>QB</td>\n",
       "      <td>JAX</td>\n",
       "      <td>9.5</td>\n",
       "      <td>54.0</td>\n",
       "      <td>29</td>\n",
       "      <td>214.495591</td>\n",
       "      <td>1.351543</td>\n",
       "      <td>0.567442</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.75</td>\n",
       "      <td>17.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>sam howell</td>\n",
       "      <td>QB</td>\n",
       "      <td>WAS</td>\n",
       "      <td>6.7</td>\n",
       "      <td>59.0</td>\n",
       "      <td>30</td>\n",
       "      <td>234.500000</td>\n",
       "      <td>1.394584</td>\n",
       "      <td>0.639448</td>\n",
       "      <td>12.501128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.25</td>\n",
       "      <td>31.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>bailey zappe</td>\n",
       "      <td>QB</td>\n",
       "      <td>NE</td>\n",
       "      <td>12.1</td>\n",
       "      <td>71.0</td>\n",
       "      <td>31</td>\n",
       "      <td>195.500000</td>\n",
       "      <td>0.615610</td>\n",
       "      <td>0.616228</td>\n",
       "      <td>5.489177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.75</td>\n",
       "      <td>27.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>2023</td>\n",
       "      <td>17</td>\n",
       "      <td>trevor siemian</td>\n",
       "      <td>QB</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>13.7</td>\n",
       "      <td>94.0</td>\n",
       "      <td>32</td>\n",
       "      <td>177.494463</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.481323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.25</td>\n",
       "      <td>20.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2005 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Season  Week             Name Position Team  Fantasy Points  Rank  \\\n",
       "0       2020     1  patrick mahomes       QB   KC            20.4   1.0   \n",
       "1       2020     1    lamar jackson       QB  BAL            27.5   2.0   \n",
       "2       2020     1     dak prescott       QB  DAL            17.6   4.0   \n",
       "3       2020     1   russell wilson       QB  SEA            31.8   5.0   \n",
       "4       2020     1   deshaun watson       QB  HOU            20.8   6.0   \n",
       "...      ...   ...              ...      ...  ...             ...   ...   \n",
       "2000    2023    17  taylor heinicke       QB  ATL            15.1  52.0   \n",
       "2001    2023    17      cj beathard       QB  JAX             9.5  54.0   \n",
       "2002    2023    17       sam howell       QB  WAS             6.7  59.0   \n",
       "2003    2023    17     bailey zappe       QB   NE            12.1  71.0   \n",
       "2004    2023    17   trevor siemian       QB  NYJ            13.7  94.0   \n",
       "\n",
       "      Position Rank  Adjusted Passing Yards Projection  \\\n",
       "0                 1                         306.500000   \n",
       "1                 2                         222.500000   \n",
       "2                 3                         285.500000   \n",
       "3                 4                         259.500000   \n",
       "4                 5                         275.500000   \n",
       "...             ...                                ...   \n",
       "2000             28                         203.494463   \n",
       "2001             29                         214.495591   \n",
       "2002             30                         234.500000   \n",
       "2003             31                         195.500000   \n",
       "2004             32                         177.494463   \n",
       "\n",
       "      Adjusted Passing Touchdowns Projection  \\\n",
       "0                                   2.460615   \n",
       "1                                   1.528860   \n",
       "2                                   2.495487   \n",
       "3                                   1.564394   \n",
       "4                                   1.574315   \n",
       "...                                      ...   \n",
       "2000                                1.304627   \n",
       "2001                                1.351543   \n",
       "2002                                1.394584   \n",
       "2003                                0.615610   \n",
       "2004                                0.500000   \n",
       "\n",
       "      Adjusted Interceptions Projection  Adjusted Rushing Yards Projection  \\\n",
       "0                                   NaN                          20.488656   \n",
       "1                                   NaN                          60.500000   \n",
       "2                                   NaN                          11.517041   \n",
       "3                                   NaN                          21.500000   \n",
       "4                                   NaN                          30.512870   \n",
       "...                                 ...                                ...   \n",
       "2000                           0.510823                          10.504409   \n",
       "2001                           0.567442                          10.500000   \n",
       "2002                           0.639448                          12.501128   \n",
       "2003                           0.616228                           5.489177   \n",
       "2004                                NaN                           7.481323   \n",
       "\n",
       "      Adjusted Receiving Yards Projection  Adjusted Receptions Projection  \\\n",
       "0                                     NaN                             NaN   \n",
       "1                                     NaN                             NaN   \n",
       "2                                     NaN                             NaN   \n",
       "3                                     NaN                             NaN   \n",
       "4                                     NaN                             NaN   \n",
       "...                                   ...                             ...   \n",
       "2000                                  NaN                             NaN   \n",
       "2001                                  NaN                             NaN   \n",
       "2002                                  NaN                             NaN   \n",
       "2003                                  NaN                             NaN   \n",
       "2004                                  NaN                             NaN   \n",
       "\n",
       "      Anytime Touchdown Probability  Location  Team Projected Score  \\\n",
       "0                          0.250000       1.0                 31.50   \n",
       "1                          0.476190       1.0                 27.25   \n",
       "2                          0.230947      -1.0                 25.50   \n",
       "3                          0.200000      -1.0                   NaN   \n",
       "4                          0.344828      -1.0                 22.00   \n",
       "...                             ...       ...                   ...   \n",
       "2000                       0.153846      -1.0                 18.25   \n",
       "2001                       0.133333       1.0                 20.75   \n",
       "2002                       0.153846       1.0                 17.25   \n",
       "2003                       0.058824      -1.0                 12.75   \n",
       "2004                            NaN      -1.0                 13.25   \n",
       "\n",
       "      Opponent Projected Score  \n",
       "0                        22.00  \n",
       "1                        20.25  \n",
       "2                        26.50  \n",
       "3                          NaN  \n",
       "4                        31.50  \n",
       "...                        ...  \n",
       "2000                     20.75  \n",
       "2001                     17.25  \n",
       "2002                     31.25  \n",
       "2003                     27.25  \n",
       "2004                     20.25  \n",
       "\n",
       "[2005 rows x 18 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "df_mod = pd.read_parquet('../../../data/model_data/model_data_single_output.parquet')\n",
    "\n",
    "df_mod = df_mod.loc[df_mod['Position'] == 'QB', :].reset_index(drop=True)\n",
    "\n",
    "df_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Position Rank</th>\n",
       "      <th>Location</th>\n",
       "      <th>Team Projected Score</th>\n",
       "      <th>Opponent Projected Score</th>\n",
       "      <th>Adjusted Passing Yards Projection</th>\n",
       "      <th>Adjusted Passing Touchdowns Projection</th>\n",
       "      <th>Adjusted Interceptions Projection</th>\n",
       "      <th>Adjusted Rushing Yards Projection</th>\n",
       "      <th>Anytime Touchdown Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.50</td>\n",
       "      <td>22.00</td>\n",
       "      <td>306.500000</td>\n",
       "      <td>2.460615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.488656</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.25</td>\n",
       "      <td>20.25</td>\n",
       "      <td>222.500000</td>\n",
       "      <td>1.528860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>0.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>25.50</td>\n",
       "      <td>26.50</td>\n",
       "      <td>285.500000</td>\n",
       "      <td>2.495487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.517041</td>\n",
       "      <td>0.230947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259.500000</td>\n",
       "      <td>1.564394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>31.50</td>\n",
       "      <td>275.500000</td>\n",
       "      <td>1.574315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.512870</td>\n",
       "      <td>0.344828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>52.0</td>\n",
       "      <td>28</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>18.25</td>\n",
       "      <td>20.75</td>\n",
       "      <td>203.494463</td>\n",
       "      <td>1.304627</td>\n",
       "      <td>0.510823</td>\n",
       "      <td>10.504409</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>54.0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.75</td>\n",
       "      <td>17.25</td>\n",
       "      <td>214.495591</td>\n",
       "      <td>1.351543</td>\n",
       "      <td>0.567442</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>59.0</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.25</td>\n",
       "      <td>31.25</td>\n",
       "      <td>234.500000</td>\n",
       "      <td>1.394584</td>\n",
       "      <td>0.639448</td>\n",
       "      <td>12.501128</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>71.0</td>\n",
       "      <td>31</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12.75</td>\n",
       "      <td>27.25</td>\n",
       "      <td>195.500000</td>\n",
       "      <td>0.615610</td>\n",
       "      <td>0.616228</td>\n",
       "      <td>5.489177</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>94.0</td>\n",
       "      <td>32</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>13.25</td>\n",
       "      <td>20.25</td>\n",
       "      <td>177.494463</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.481323</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2005 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rank  Position Rank  Location  Team Projected Score  \\\n",
       "0      1.0              1       1.0                 31.50   \n",
       "1      2.0              2       1.0                 27.25   \n",
       "2      4.0              3      -1.0                 25.50   \n",
       "3      5.0              4      -1.0                   NaN   \n",
       "4      6.0              5      -1.0                 22.00   \n",
       "...    ...            ...       ...                   ...   \n",
       "2000  52.0             28      -1.0                 18.25   \n",
       "2001  54.0             29       1.0                 20.75   \n",
       "2002  59.0             30       1.0                 17.25   \n",
       "2003  71.0             31      -1.0                 12.75   \n",
       "2004  94.0             32      -1.0                 13.25   \n",
       "\n",
       "      Opponent Projected Score  Adjusted Passing Yards Projection  \\\n",
       "0                        22.00                         306.500000   \n",
       "1                        20.25                         222.500000   \n",
       "2                        26.50                         285.500000   \n",
       "3                          NaN                         259.500000   \n",
       "4                        31.50                         275.500000   \n",
       "...                        ...                                ...   \n",
       "2000                     20.75                         203.494463   \n",
       "2001                     17.25                         214.495591   \n",
       "2002                     31.25                         234.500000   \n",
       "2003                     27.25                         195.500000   \n",
       "2004                     20.25                         177.494463   \n",
       "\n",
       "      Adjusted Passing Touchdowns Projection  \\\n",
       "0                                   2.460615   \n",
       "1                                   1.528860   \n",
       "2                                   2.495487   \n",
       "3                                   1.564394   \n",
       "4                                   1.574315   \n",
       "...                                      ...   \n",
       "2000                                1.304627   \n",
       "2001                                1.351543   \n",
       "2002                                1.394584   \n",
       "2003                                0.615610   \n",
       "2004                                0.500000   \n",
       "\n",
       "      Adjusted Interceptions Projection  Adjusted Rushing Yards Projection  \\\n",
       "0                                   NaN                          20.488656   \n",
       "1                                   NaN                          60.500000   \n",
       "2                                   NaN                          11.517041   \n",
       "3                                   NaN                          21.500000   \n",
       "4                                   NaN                          30.512870   \n",
       "...                                 ...                                ...   \n",
       "2000                           0.510823                          10.504409   \n",
       "2001                           0.567442                          10.500000   \n",
       "2002                           0.639448                          12.501128   \n",
       "2003                           0.616228                           5.489177   \n",
       "2004                                NaN                           7.481323   \n",
       "\n",
       "      Anytime Touchdown Probability  \n",
       "0                          0.250000  \n",
       "1                          0.476190  \n",
       "2                          0.230947  \n",
       "3                          0.200000  \n",
       "4                          0.344828  \n",
       "...                             ...  \n",
       "2000                       0.153846  \n",
       "2001                       0.133333  \n",
       "2002                       0.153846  \n",
       "2003                       0.058824  \n",
       "2004                            NaN  \n",
       "\n",
       "[2005 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_mod[[\n",
    "    'Rank',\n",
    "    'Position Rank',\n",
    "    'Location',\n",
    "    'Team Projected Score',\n",
    "    'Opponent Projected Score',\n",
    "    'Adjusted Passing Yards Projection',\n",
    "    'Adjusted Passing Touchdowns Projection',\n",
    "    'Adjusted Interceptions Projection',\n",
    "    'Adjusted Rushing Yards Projection',\n",
    "    'Anytime Touchdown Probability',\n",
    "]].copy()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       20\n",
       "1       28\n",
       "2       18\n",
       "3       32\n",
       "4       21\n",
       "        ..\n",
       "2000    15\n",
       "2001    10\n",
       "2002     7\n",
       "2003    12\n",
       "2004    14\n",
       "Name: Fantasy Points, Length: 2005, dtype: int32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_rounded = df_mod['Fantasy Points'].round().astype(int)\n",
    "\n",
    "points_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-4 Fantasy Points</th>\n",
       "      <th>-3 Fantasy Points</th>\n",
       "      <th>-2 Fantasy Points</th>\n",
       "      <th>-1 Fantasy Points</th>\n",
       "      <th>0 Fantasy Points</th>\n",
       "      <th>1 Fantasy Points</th>\n",
       "      <th>2 Fantasy Points</th>\n",
       "      <th>3 Fantasy Points</th>\n",
       "      <th>4 Fantasy Points</th>\n",
       "      <th>5 Fantasy Points</th>\n",
       "      <th>6 Fantasy Points</th>\n",
       "      <th>7 Fantasy Points</th>\n",
       "      <th>8 Fantasy Points</th>\n",
       "      <th>9 Fantasy Points</th>\n",
       "      <th>10 Fantasy Points</th>\n",
       "      <th>11 Fantasy Points</th>\n",
       "      <th>12 Fantasy Points</th>\n",
       "      <th>13 Fantasy Points</th>\n",
       "      <th>14 Fantasy Points</th>\n",
       "      <th>15 Fantasy Points</th>\n",
       "      <th>16 Fantasy Points</th>\n",
       "      <th>17 Fantasy Points</th>\n",
       "      <th>18 Fantasy Points</th>\n",
       "      <th>19 Fantasy Points</th>\n",
       "      <th>20 Fantasy Points</th>\n",
       "      <th>21 Fantasy Points</th>\n",
       "      <th>22 Fantasy Points</th>\n",
       "      <th>23 Fantasy Points</th>\n",
       "      <th>24 Fantasy Points</th>\n",
       "      <th>25 Fantasy Points</th>\n",
       "      <th>26 Fantasy Points</th>\n",
       "      <th>27 Fantasy Points</th>\n",
       "      <th>28 Fantasy Points</th>\n",
       "      <th>29 Fantasy Points</th>\n",
       "      <th>30 Fantasy Points</th>\n",
       "      <th>31 Fantasy Points</th>\n",
       "      <th>32 Fantasy Points</th>\n",
       "      <th>33 Fantasy Points</th>\n",
       "      <th>34 Fantasy Points</th>\n",
       "      <th>35 Fantasy Points</th>\n",
       "      <th>36 Fantasy Points</th>\n",
       "      <th>37 Fantasy Points</th>\n",
       "      <th>38 Fantasy Points</th>\n",
       "      <th>39 Fantasy Points</th>\n",
       "      <th>40 Fantasy Points</th>\n",
       "      <th>41 Fantasy Points</th>\n",
       "      <th>42 Fantasy Points</th>\n",
       "      <th>43 Fantasy Points</th>\n",
       "      <th>44 Fantasy Points</th>\n",
       "      <th>45 Fantasy Points</th>\n",
       "      <th>46 Fantasy Points</th>\n",
       "      <th>47 Fantasy Points</th>\n",
       "      <th>48 Fantasy Points</th>\n",
       "      <th>49 Fantasy Points</th>\n",
       "      <th>50 Fantasy Points</th>\n",
       "      <th>51 Fantasy Points</th>\n",
       "      <th>52 Fantasy Points</th>\n",
       "      <th>53 Fantasy Points</th>\n",
       "      <th>54 Fantasy Points</th>\n",
       "      <th>55 Fantasy Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2005 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      -4 Fantasy Points  -3 Fantasy Points  -2 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      -1 Fantasy Points  0 Fantasy Points  1 Fantasy Points  2 Fantasy Points  \\\n",
       "0                   0.0               0.0               0.0               0.0   \n",
       "1                   0.0               0.0               0.0               0.0   \n",
       "2                   0.0               0.0               0.0               0.0   \n",
       "3                   0.0               0.0               0.0               0.0   \n",
       "4                   0.0               0.0               0.0               0.0   \n",
       "...                 ...               ...               ...               ...   \n",
       "2000                0.0               0.0               0.0               0.0   \n",
       "2001                0.0               0.0               0.0               0.0   \n",
       "2002                0.0               0.0               0.0               0.0   \n",
       "2003                0.0               0.0               0.0               0.0   \n",
       "2004                0.0               0.0               0.0               0.0   \n",
       "\n",
       "      3 Fantasy Points  4 Fantasy Points  5 Fantasy Points  6 Fantasy Points  \\\n",
       "0                  0.0               0.0               0.0               0.0   \n",
       "1                  0.0               0.0               0.0               0.0   \n",
       "2                  0.0               0.0               0.0               0.0   \n",
       "3                  0.0               0.0               0.0               0.0   \n",
       "4                  0.0               0.0               0.0               0.0   \n",
       "...                ...               ...               ...               ...   \n",
       "2000               0.0               0.0               0.0               0.0   \n",
       "2001               0.0               0.0               0.0               0.0   \n",
       "2002               0.0               0.0               0.0               0.0   \n",
       "2003               0.0               0.0               0.0               0.0   \n",
       "2004               0.0               0.0               0.0               0.0   \n",
       "\n",
       "      7 Fantasy Points  8 Fantasy Points  9 Fantasy Points  10 Fantasy Points  \\\n",
       "0                  0.0               0.0               0.0                0.0   \n",
       "1                  0.0               0.0               0.0                0.0   \n",
       "2                  0.0               0.0               0.0                0.0   \n",
       "3                  0.0               0.0               0.0                0.0   \n",
       "4                  0.0               0.0               0.0                0.0   \n",
       "...                ...               ...               ...                ...   \n",
       "2000               0.0               0.0               0.0                0.0   \n",
       "2001               0.0               0.0               0.0                1.0   \n",
       "2002               1.0               0.0               0.0                0.0   \n",
       "2003               0.0               0.0               0.0                0.0   \n",
       "2004               0.0               0.0               0.0                0.0   \n",
       "\n",
       "      11 Fantasy Points  12 Fantasy Points  13 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                1.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      14 Fantasy Points  15 Fantasy Points  16 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                1.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                1.0                0.0                0.0   \n",
       "\n",
       "      17 Fantasy Points  18 Fantasy Points  19 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                1.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      20 Fantasy Points  21 Fantasy Points  22 Fantasy Points  \\\n",
       "0                   1.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                1.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      23 Fantasy Points  24 Fantasy Points  25 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      26 Fantasy Points  27 Fantasy Points  28 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                1.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      29 Fantasy Points  30 Fantasy Points  31 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      32 Fantasy Points  33 Fantasy Points  34 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   1.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      35 Fantasy Points  36 Fantasy Points  37 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      38 Fantasy Points  39 Fantasy Points  40 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      41 Fantasy Points  42 Fantasy Points  43 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      44 Fantasy Points  45 Fantasy Points  46 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      47 Fantasy Points  48 Fantasy Points  49 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      50 Fantasy Points  51 Fantasy Points  52 Fantasy Points  \\\n",
       "0                   0.0                0.0                0.0   \n",
       "1                   0.0                0.0                0.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   0.0                0.0                0.0   \n",
       "4                   0.0                0.0                0.0   \n",
       "...                 ...                ...                ...   \n",
       "2000                0.0                0.0                0.0   \n",
       "2001                0.0                0.0                0.0   \n",
       "2002                0.0                0.0                0.0   \n",
       "2003                0.0                0.0                0.0   \n",
       "2004                0.0                0.0                0.0   \n",
       "\n",
       "      53 Fantasy Points  54 Fantasy Points  55 Fantasy Points  \n",
       "0                   0.0                0.0                0.0  \n",
       "1                   0.0                0.0                0.0  \n",
       "2                   0.0                0.0                0.0  \n",
       "3                   0.0                0.0                0.0  \n",
       "4                   0.0                0.0                0.0  \n",
       "...                 ...                ...                ...  \n",
       "2000                0.0                0.0                0.0  \n",
       "2001                0.0                0.0                0.0  \n",
       "2002                0.0                0.0                0.0  \n",
       "2003                0.0                0.0                0.0  \n",
       "2004                0.0                0.0                0.0  \n",
       "\n",
       "[2005 rows x 60 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# first column: -5 points scored\n",
    "# last column: 55 points scored\n",
    "y = np.zeros(shape=(points_rounded.shape[0], 60))\n",
    "\n",
    "for i in range(y.shape[0]):\n",
    "    y[i, points_rounded.iloc[i] + 4] = 1\n",
    "\n",
    "y = pd.DataFrame(\n",
    "    y,\n",
    "    columns=[f\"{i - 4} Fantasy Points\" for i in range(y.shape[1])]\n",
    ")\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2020 Week 1\n",
       "1        2020 Week 1\n",
       "2        2020 Week 1\n",
       "3        2020 Week 1\n",
       "4        2020 Week 1\n",
       "            ...     \n",
       "2000    2023 Week 17\n",
       "2001    2023 Week 17\n",
       "2002    2023 Week 17\n",
       "2003    2023 Week 17\n",
       "2004    2023 Week 17\n",
       "Length: 2005, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = df_mod['Season'].astype(str) + ' Week ' + df_mod['Week'].astype(str)\n",
    "\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold ID</th>\n",
       "      <th>Season Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022 Week 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2021 Week 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2021 Week 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2023 Week 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2021 Week 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2</td>\n",
       "      <td>2020 Week 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2</td>\n",
       "      <td>2023 Week 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2</td>\n",
       "      <td>2021 Week 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2</td>\n",
       "      <td>2022 Week 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2</td>\n",
       "      <td>2022 Week 12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fold ID   Season Week\n",
       "0         0   2022 Week 3\n",
       "1         0   2021 Week 7\n",
       "2         0   2021 Week 1\n",
       "3         0  2023 Week 16\n",
       "4         0  2021 Week 11\n",
       "..      ...           ...\n",
       "62        2  2020 Week 15\n",
       "63        2  2023 Week 14\n",
       "64        2   2021 Week 5\n",
       "65        2   2022 Week 6\n",
       "66        2  2022 Week 12\n",
       "\n",
       "[67 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_folds = pd.read_parquet('../../../data/model_data/folds.parquet')\n",
    "\n",
    "df_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1102, 6) (215, 6) (688, 6) (1102, 60) (215, 60) (688, 60) (688, 18)\n",
      "(1150, 6) (211, 6) (644, 6) (1150, 60) (211, 60) (644, 60) (644, 18)\n",
      "(1129, 6) (203, 6) (673, 6) (1129, 60) (203, 60) (673, 60) (673, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# splitter = GroupKFold(n_splits=3)\n",
    "\n",
    "cv_data = []\n",
    "# for is_indexes, oos_indexes in splitter.split(X=X, y=y, groups=groups):\n",
    "for fold in df_folds['Fold ID'].unique():\n",
    "    oos_season_week = df_folds.loc[df_folds['Fold ID'] == fold, 'Season Week']\n",
    "    is_indexes = df_mod.loc[~groups.isin(oos_season_week), :].index\n",
    "    oos_indexes = df_mod.loc[groups.isin(oos_season_week), :].index\n",
    "    # split\n",
    "    X_is = X.iloc[is_indexes]\n",
    "    X_oos = X.iloc[oos_indexes]\n",
    "\n",
    "    y_is = y.iloc[is_indexes]\n",
    "    y_oos = y.iloc[oos_indexes]\n",
    "\n",
    "    groups_is = groups.iloc[is_indexes]\n",
    "    df_mod_oos = df_mod.iloc[oos_indexes]\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=22)\n",
    "    for train_indexes, val_indexes in gss.split(X=X_is, y=y_is, groups=groups_is):\n",
    "            X_train = X_is.iloc[train_indexes]\n",
    "            X_val = X_is.iloc[val_indexes]\n",
    "\n",
    "            y_train = y_is.iloc[train_indexes]\n",
    "            y_val = y_is.iloc[val_indexes]\n",
    "\n",
    "    # impute\n",
    "    scaler = StandardScaler()\n",
    "    imputer = IterativeImputer(initial_strategy='median', max_iter=100)\n",
    "\n",
    "    X_train_fill_na = imputer.fit_transform(scaler.fit_transform(X_train))\n",
    "    X_train[X_train.columns] = scaler.inverse_transform(X_train_fill_na).copy()\n",
    "    X_train['Prop Bets Projection'] = (\n",
    "        X_train['Adjusted Passing Yards Projection']*0.04 + \n",
    "        X_train['Adjusted Passing Touchdowns Projection']*4 + \n",
    "        X_train['Adjusted Interceptions Projection']*(-2) + \n",
    "        X_train['Adjusted Rushing Yards Projection']*0.1 +\n",
    "        X_train['Anytime Touchdown Probability']*6\n",
    "    )\n",
    "\n",
    "    scaler2 = MinMaxScaler(clip=True)  # maybe normalize and clip instead of standardize?\n",
    "    scaler3 = StandardScaler()\n",
    "    X_train[X_train.columns] = scaler3.fit_transform(scaler2.fit_transform(X_train)).copy()\n",
    "\n",
    "    X_val_fill_na = imputer.transform(scaler.transform(X_val))\n",
    "    X_val[X_val.columns] = scaler.inverse_transform(X_val_fill_na).copy()\n",
    "    X_val['Prop Bets Projection'] = (\n",
    "        X_val['Adjusted Passing Yards Projection']*0.04 + \n",
    "        X_val['Adjusted Passing Touchdowns Projection']*4 + \n",
    "        X_val['Adjusted Interceptions Projection']*(-2) + \n",
    "        X_val['Adjusted Rushing Yards Projection']*0.1 +\n",
    "        X_val['Anytime Touchdown Probability']*6\n",
    "    )\n",
    "\n",
    "    X_val[X_val.columns] = scaler3.transform(scaler2.transform(X_val)).copy()\n",
    "\n",
    "    X_oos_fill_na = imputer.transform(scaler.transform(X_oos))\n",
    "    X_oos[X_oos.columns] = scaler.inverse_transform(X_oos_fill_na).copy()\n",
    "    X_oos['Prop Bets Projection'] = (\n",
    "        X_oos['Adjusted Passing Yards Projection']*0.04 + \n",
    "        X_oos['Adjusted Passing Touchdowns Projection']*4 + \n",
    "        X_oos['Adjusted Interceptions Projection']*(-2) + \n",
    "        X_oos['Adjusted Rushing Yards Projection']*0.1 +\n",
    "        X_oos['Anytime Touchdown Probability']*6\n",
    "    )\n",
    "\n",
    "    X_oos[X_oos.columns] = scaler3.transform(scaler2.transform(X_oos)).copy()\n",
    "\n",
    "    X_train.drop(columns=['Adjusted Passing Yards Projection', 'Adjusted Passing Touchdowns Projection', 'Adjusted Interceptions Projection', 'Adjusted Rushing Yards Projection', 'Anytime Touchdown Probability'], inplace=True)\n",
    "    X_val.drop(columns=['Adjusted Passing Yards Projection', 'Adjusted Passing Touchdowns Projection', 'Adjusted Interceptions Projection', 'Adjusted Rushing Yards Projection', 'Anytime Touchdown Probability'], inplace=True)\n",
    "    X_oos.drop(columns=['Adjusted Passing Yards Projection', 'Adjusted Passing Touchdowns Projection', 'Adjusted Interceptions Projection', 'Adjusted Rushing Yards Projection', 'Anytime Touchdown Probability'], inplace=True)\n",
    "\n",
    "    cv_data.append((X_train, X_val, X_oos, y_train, y_val, y_oos, df_mod_oos))\n",
    "\n",
    "for X_train, X_val, X_oos, y_train, y_val, y_oos, df_mod_oos in cv_data:\n",
    "    print(X_train.shape, X_val.shape, X_oos.shape, y_train.shape, y_val.shape, y_oos.shape, df_mod_oos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 10)]              0         \n",
      "                                                                 \n",
      " hidden_1 (Dense)            (None, 128)               1408      \n",
      "                                                                 \n",
      " hidden_2 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " outputs (Dense)             (None, 60)                1980      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,516\n",
      "Trainable params: 7,516\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotNormal\n",
    "from tensorflow.config.experimental import enable_op_determinism\n",
    "from tensorflow.random import set_seed\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "def build_and_compile_model(input_shape: tuple, output_shape: int, hidden_layer_neurons: list, l1s: list, l2s: list, learning_rate: float):\n",
    "    enable_op_determinism()\n",
    "    set_seed(22)\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='input')\n",
    "\n",
    "    h = inputs\n",
    "    for i, neurons in enumerate(hidden_layer_neurons):\n",
    "        h = Dense(\n",
    "            neurons, \n",
    "            activation='relu', \n",
    "            kernel_initializer=GlorotNormal(seed=22), \n",
    "            kernel_regularizer=L1L2(l1=l1s[i], l2=l2s[i]), \n",
    "            name=f\"hidden_{i+1}\"\n",
    "        )(h)\n",
    "\n",
    "    outputs = Dense(output_shape, activation='softmax', kernel_initializer=GlorotNormal(seed=22), name='outputs')(h)\n",
    "\n",
    "    mod = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    mod.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return mod\n",
    "\n",
    "mod = build_and_compile_model(X.shape[1:], y.shape[1], [128, 32], [0, 0.01], [0, 0.01], learning_rate=0.001)\n",
    "\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhugh\\anaconda3\\envs\\clean2\\lib\\site-packages\\optuna\\_experimental.py:30: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2024-10-21 16:26:42,450] A new study created in memory with name: no-name-bbefd5c2-a559-4456-be24-16f2b2d11b75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbabd83d0e84e28b5468157916d0948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:26:58,512] Trial 0 finished with value: 3.4426650602865174 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.022040451663772567, 'hidden_layer_1_l2': 0.08119509205386867, 'learning_rate': 0.01094741868844976, 'batch_size': 2048}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:27:08,457] Trial 1 finished with value: 3.5349310146883703 and parameters: {'n_hidden_layers': 3, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.058428964315689495, 'hidden_layer_1_l2': 0.07026355188654519, 'hidden_layer_2_neurons': 128, 'hidden_layer_2_l1': 0.08749277452777171, 'hidden_layer_2_l2': 0.07449320773686179, 'hidden_layer_3_neurons': 256, 'hidden_layer_3_l1': 0.009588460991804116, 'hidden_layer_3_l2': 0.004521011190769353, 'learning_rate': 0.07163458468004151, 'batch_size': 1024}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:27:22,823] Trial 2 finished with value: 3.49984469999421 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 64, 'hidden_layer_1_l1': 0.09115369973853776, 'hidden_layer_1_l2': 0.008625567976924198, 'hidden_layer_2_neurons': 2048, 'hidden_layer_2_l1': 0.0005512411903590642, 'hidden_layer_2_l2': 0.03576431697413079, 'learning_rate': 0.09578334240413691, 'batch_size': 1024}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:27:37,716] Trial 3 finished with value: 3.45918077863992 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 1024, 'hidden_layer_1_l1': 0.08620878124622662, 'hidden_layer_1_l2': 0.06351961895274903, 'learning_rate': 0.07275008619568284, 'batch_size': 4096}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:27:46,956] Trial 4 finished with value: 3.461387403640985 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 1024, 'hidden_layer_1_l1': 0.02024276263846314, 'hidden_layer_1_l2': 0.08721894394962987, 'learning_rate': 0.06424371980220045, 'batch_size': 1024}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:28:00,010] Trial 5 finished with value: 3.451199373657503 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 32, 'hidden_layer_1_l1': 0.05653980354893057, 'hidden_layer_1_l2': 0.00345150171194456, 'learning_rate': 0.0885147769978044, 'batch_size': 16384}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:28:12,673] Trial 6 finished with value: 3.5126674827841446 and parameters: {'n_hidden_layers': 3, 'hidden_layer_1_neurons': 256, 'hidden_layer_1_l1': 0.07564442330254927, 'hidden_layer_1_l2': 0.045177882434907994, 'hidden_layer_2_neurons': 64, 'hidden_layer_2_l1': 0.08362472880832089, 'hidden_layer_2_l2': 0.05103829122196473, 'hidden_layer_3_neurons': 128, 'hidden_layer_3_l1': 0.06951388599043494, 'hidden_layer_3_l2': 0.0051964104142108065, 'learning_rate': 0.09447015559422758, 'batch_size': 2048}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:28:27,094] Trial 7 finished with value: 3.5225739675903798 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 32, 'hidden_layer_1_l1': 0.08592805490779229, 'hidden_layer_1_l2': 0.0407904418229531, 'hidden_layer_2_neurons': 64, 'hidden_layer_2_l1': 0.025309932195697216, 'hidden_layer_2_l2': 0.03508986025005804, 'learning_rate': 0.06905762708317247, 'batch_size': 1024}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:28:34,919] Trial 8 finished with value: 3.478570513902335 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 32, 'hidden_layer_1_l1': 0.004671699281628905, 'hidden_layer_1_l2': 0.056923526796298245, 'learning_rate': 0.09827798409489276, 'batch_size': 1024}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:28:46,041] Trial 9 finished with value: 3.519396630514838 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 256, 'hidden_layer_1_l1': 0.0004424057058423636, 'hidden_layer_1_l2': 0.06813144847420316, 'hidden_layer_2_neurons': 64, 'hidden_layer_2_l1': 0.08249019408071431, 'hidden_layer_2_l2': 0.04359087388902699, 'learning_rate': 0.05221946484311475, 'batch_size': 1024}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:28:59,264] Trial 10 finished with value: 3.44874070687572 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.020822305720339832, 'hidden_layer_1_l2': 0.09105201059054474, 'learning_rate': 0.03633285507832701, 'batch_size': 2048}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:29:15,601] Trial 11 finished with value: 3.4479894882703586 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 1024, 'hidden_layer_1_l1': 0.02212847698075585, 'hidden_layer_1_l2': 0.0710101001486778, 'learning_rate': 0.01576495928001541, 'batch_size': 2048}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:29:34,745] Trial 12 finished with value: 3.447015465239861 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 1024, 'hidden_layer_1_l1': 0.041963417671990126, 'hidden_layer_1_l2': 0.07225432618359447, 'learning_rate': 0.014277031177754917, 'batch_size': 32768}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:29:56,294] Trial 13 finished with value: 3.4572210620898325 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 1024, 'hidden_layer_1_l1': 0.0741475656604011, 'hidden_layer_1_l2': 0.056528618157062116, 'learning_rate': 0.012031330549462317, 'batch_size': 32768}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:30:13,100] Trial 14 finished with value: 3.448442735329791 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 512, 'hidden_layer_1_l1': 0.027150651821126547, 'hidden_layer_1_l2': 0.04933021318916059, 'learning_rate': 0.018829579007387055, 'batch_size': 32768}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:30:39,471] Trial 15 finished with value: 3.517153899111068 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 2048, 'hidden_layer_1_l1': 0.05350830228012085, 'hidden_layer_1_l2': 0.0933264357253227, 'hidden_layer_2_neurons': 1024, 'hidden_layer_2_l1': 0.049366008343461366, 'hidden_layer_2_l2': 0.00042701546659963674, 'learning_rate': 0.02384221494590049, 'batch_size': 32768}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:30:52,188] Trial 16 finished with value: 3.516329182114452 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 1024, 'hidden_layer_1_l1': 0.048287983779457394, 'hidden_layer_1_l2': 0.06637624365317128, 'hidden_layer_2_neurons': 256, 'hidden_layer_2_l1': 0.04731723274505317, 'hidden_layer_2_l2': 0.09481379950283156, 'learning_rate': 0.02264983827220762, 'batch_size': 16384}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:31:11,785] Trial 17 finished with value: 3.50626117411113 and parameters: {'n_hidden_layers': 2, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.0213063154964818, 'hidden_layer_1_l2': 0.053089394849644145, 'hidden_layer_2_neurons': 32, 'hidden_layer_2_l1': 0.023800435800129673, 'hidden_layer_2_l2': 0.07173119339114593, 'learning_rate': 0.016242803668226925, 'batch_size': 4096}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:31:24,815] Trial 18 finished with value: 3.4456049806952604 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.018261064697137848, 'hidden_layer_1_l2': 0.028523222925107283, 'learning_rate': 0.017308611881211997, 'batch_size': 2048}. Best is trial 0 with value: 3.4426650602865174.\n",
      "(2005, 60) (2005, 60)\n",
      "[I 2024-10-21 16:31:35,778] Trial 19 finished with value: 3.455574252154203 and parameters: {'n_hidden_layers': 1, 'hidden_layer_1_neurons': 128, 'hidden_layer_1_l1': 0.010073963708808304, 'hidden_layer_1_l2': 0.03121956071895334, 'learning_rate': 0.03462201626982987, 'batch_size': 2048}. Best is trial 0 with value: 3.4426650602865174.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_hidden_layers': 1,\n",
       " 'hidden_layer_1_neurons': 128,\n",
       " 'hidden_layer_1_l1': 0.022040451663772567,\n",
       " 'hidden_layer_1_l2': 0.08119509205386867,\n",
       " 'learning_rate': 0.01094741868844976,\n",
       " 'batch_size': 2048}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import optuna\n",
    "\n",
    "def objective(trial, cv_data=cv_data):\n",
    "# model tuning\n",
    "    n_hidden_layers = trial.suggest_int(f\"n_hidden_layers\", 1, 3)\n",
    "\n",
    "    hidden_layer_neurons = []\n",
    "    l1s = []\n",
    "    l2s = []\n",
    "    for i in range(n_hidden_layers):\n",
    "        hidden_layer_neurons.append(trial.suggest_categorical(f\"hidden_layer_{i+1}_neurons\", [2**n for n in range(5, 12)]))  # change to (3, 12)\n",
    "        # hidden_layer_neurons.append(trial.suggest_categorical(f\"hidden_layer_{i+1}_neurons\", [2**n for n in range(4, 9)]))  # bump this up\n",
    "        l1s.append(trial.suggest_float(f\"hidden_layer_{i+1}_l1\", 0.0, 0.1))  # change to (0.0, 0.05)\n",
    "        l2s.append(trial.suggest_float(f\"hidden_layer_{i+1}_l2\", 0.0, 0.1))  # change to (0.0, 0.20)\n",
    "\n",
    "    learning_rate = trial.suggest_float(f\"learning_rate\", 0.01, 0.10)\n",
    "    batch_size = trial.suggest_categorical(f\"batch_size\", [2**n for n in range(10, 16)])\n",
    "    # batch_size = trial.suggest_categorical(f\"batch_size\", [2**n for n in range(4, 12)])  # bump this up\n",
    "\n",
    "    # cross validation\n",
    "    y_oos_list = []\n",
    "    y_pred_list = []\n",
    "    for X_train, X_val, X_oos, y_train, y_val, y_oos, df_mod_oos in cv_data:\n",
    "        # make sure to build mod in loop to prevent history\n",
    "        mod = build_and_compile_model(X_train.shape[1:], y_train.shape[1], hidden_layer_neurons, l1s, l2s, learning_rate)\n",
    "\n",
    "        mod.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=500,\n",
    "            callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        y_oos_list.append(y_oos)\n",
    "        y_pred_list.append(mod.predict(X_oos, verbose=0))\n",
    "\n",
    "\n",
    "    y_oos_concat = np.vstack(y_oos_list)\n",
    "    y_pred_concat = np.vstack(y_pred_list)\n",
    "\n",
    "    print(y_oos_concat.shape, y_pred_concat.shape)\n",
    "\n",
    "    return log_loss(y_oos_concat, y_pred_concat)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=22, n_startup_trials=10, multivariate=True, warn_independent_sampling=False))\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4426650602865174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.7775 - accuracy: 0.0227 - val_loss: 6.3311 - val_accuracy: 0.0512\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6.3154 - accuracy: 0.0381 - val_loss: 5.9540 - val_accuracy: 0.0558\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.9199 - accuracy: 0.0454 - val_loss: 5.6324 - val_accuracy: 0.0558\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.5802 - accuracy: 0.0554 - val_loss: 5.3531 - val_accuracy: 0.0651\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5.2839 - accuracy: 0.0535 - val_loss: 5.1041 - val_accuracy: 0.0558\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5.0207 - accuracy: 0.0581 - val_loss: 4.8873 - val_accuracy: 0.0558\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.7931 - accuracy: 0.0626 - val_loss: 4.6978 - val_accuracy: 0.0605\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4.5949 - accuracy: 0.0672 - val_loss: 4.5303 - val_accuracy: 0.0558\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.4211 - accuracy: 0.0653 - val_loss: 4.3846 - val_accuracy: 0.0512\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4.2707 - accuracy: 0.0617 - val_loss: 4.2585 - val_accuracy: 0.0512\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.1427 - accuracy: 0.0572 - val_loss: 4.1508 - val_accuracy: 0.0651\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.0364 - accuracy: 0.0563 - val_loss: 4.0629 - val_accuracy: 0.0558\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.9522 - accuracy: 0.0572 - val_loss: 3.9900 - val_accuracy: 0.0512\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.8856 - accuracy: 0.0681 - val_loss: 3.9309 - val_accuracy: 0.0605\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.8355 - accuracy: 0.0681 - val_loss: 3.8797 - val_accuracy: 0.0605\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.7951 - accuracy: 0.0672 - val_loss: 3.8312 - val_accuracy: 0.0605\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.7587 - accuracy: 0.0635 - val_loss: 3.7870 - val_accuracy: 0.0558\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.7266 - accuracy: 0.0644 - val_loss: 3.7446 - val_accuracy: 0.0558\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.6961 - accuracy: 0.0653 - val_loss: 3.7037 - val_accuracy: 0.0651\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.6666 - accuracy: 0.0681 - val_loss: 3.6697 - val_accuracy: 0.0651\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.6429 - accuracy: 0.0717 - val_loss: 3.6425 - val_accuracy: 0.0651\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.6240 - accuracy: 0.0717 - val_loss: 3.6245 - val_accuracy: 0.0698\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.6115 - accuracy: 0.0690 - val_loss: 3.6119 - val_accuracy: 0.0791\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 3.6020 - accuracy: 0.0644 - val_loss: 3.6006 - val_accuracy: 0.0837\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.5917 - accuracy: 0.0617 - val_loss: 3.5930 - val_accuracy: 0.0837\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.5834 - accuracy: 0.0626 - val_loss: 3.5873 - val_accuracy: 0.0930\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.5754 - accuracy: 0.0635 - val_loss: 3.5840 - val_accuracy: 0.0930\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.5687 - accuracy: 0.0653 - val_loss: 3.5792 - val_accuracy: 0.0837\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.5598 - accuracy: 0.0653 - val_loss: 3.5748 - val_accuracy: 0.0837\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.5507 - accuracy: 0.0681 - val_loss: 3.5718 - val_accuracy: 0.0884\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.5435 - accuracy: 0.0662 - val_loss: 3.5670 - val_accuracy: 0.0837\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5345 - accuracy: 0.0672 - val_loss: 3.5617 - val_accuracy: 0.0884\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5249 - accuracy: 0.0635 - val_loss: 3.5554 - val_accuracy: 0.0977\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5142 - accuracy: 0.0626 - val_loss: 3.5492 - val_accuracy: 0.0837\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.5037 - accuracy: 0.0590 - val_loss: 3.5460 - val_accuracy: 0.0744\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4966 - accuracy: 0.0608 - val_loss: 3.5452 - val_accuracy: 0.0698\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4921 - accuracy: 0.0662 - val_loss: 3.5444 - val_accuracy: 0.0651\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4881 - accuracy: 0.0653 - val_loss: 3.5424 - val_accuracy: 0.0605\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4837 - accuracy: 0.0681 - val_loss: 3.5407 - val_accuracy: 0.0605\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4802 - accuracy: 0.0662 - val_loss: 3.5388 - val_accuracy: 0.0605\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4775 - accuracy: 0.0653 - val_loss: 3.5362 - val_accuracy: 0.0558\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4747 - accuracy: 0.0672 - val_loss: 3.5336 - val_accuracy: 0.0605\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.4720 - accuracy: 0.0672 - val_loss: 3.5304 - val_accuracy: 0.0744\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4686 - accuracy: 0.0662 - val_loss: 3.5261 - val_accuracy: 0.0698\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4643 - accuracy: 0.0626 - val_loss: 3.5221 - val_accuracy: 0.0744\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4603 - accuracy: 0.0672 - val_loss: 3.5196 - val_accuracy: 0.0698\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4576 - accuracy: 0.0672 - val_loss: 3.5177 - val_accuracy: 0.0558\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 3.4553 - accuracy: 0.0662 - val_loss: 3.5162 - val_accuracy: 0.0512\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.4539 - accuracy: 0.0672 - val_loss: 3.5131 - val_accuracy: 0.0605\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4512 - accuracy: 0.0653 - val_loss: 3.5101 - val_accuracy: 0.0605\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4488 - accuracy: 0.0644 - val_loss: 3.5060 - val_accuracy: 0.0605\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4454 - accuracy: 0.0617 - val_loss: 3.5030 - val_accuracy: 0.0744\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4434 - accuracy: 0.0617 - val_loss: 3.5006 - val_accuracy: 0.0837\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4421 - accuracy: 0.0599 - val_loss: 3.4980 - val_accuracy: 0.0837\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4404 - accuracy: 0.0608 - val_loss: 3.4949 - val_accuracy: 0.0698\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4380 - accuracy: 0.0617 - val_loss: 3.4932 - val_accuracy: 0.0698\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4361 - accuracy: 0.0653 - val_loss: 3.4927 - val_accuracy: 0.0651\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4346 - accuracy: 0.0635 - val_loss: 3.4933 - val_accuracy: 0.0558\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4336 - accuracy: 0.0635 - val_loss: 3.4949 - val_accuracy: 0.0651\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4331 - accuracy: 0.0644 - val_loss: 3.4958 - val_accuracy: 0.0698\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4316 - accuracy: 0.0617 - val_loss: 3.4972 - val_accuracy: 0.0698\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4306 - accuracy: 0.0608 - val_loss: 3.4975 - val_accuracy: 0.0651\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4291 - accuracy: 0.0635 - val_loss: 3.4967 - val_accuracy: 0.0698\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4273 - accuracy: 0.0635 - val_loss: 3.4954 - val_accuracy: 0.0605\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4253 - accuracy: 0.0653 - val_loss: 3.4964 - val_accuracy: 0.0558\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4250 - accuracy: 0.0626 - val_loss: 3.4968 - val_accuracy: 0.0558\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.4238 - accuracy: 0.0608 - val_loss: 3.4966 - val_accuracy: 0.0558\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 833ms/step - loss: 6.7769 - accuracy: 0.0174 - val_loss: 6.3342 - val_accuracy: 0.0284\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 6.3160 - accuracy: 0.0374 - val_loss: 5.9595 - val_accuracy: 0.0190\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5.9223 - accuracy: 0.0400 - val_loss: 5.6381 - val_accuracy: 0.0190\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.5847 - accuracy: 0.0478 - val_loss: 5.3572 - val_accuracy: 0.0237\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5.2896 - accuracy: 0.0496 - val_loss: 5.1071 - val_accuracy: 0.0332\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.0271 - accuracy: 0.0565 - val_loss: 4.8906 - val_accuracy: 0.0332\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.7998 - accuracy: 0.0591 - val_loss: 4.7001 - val_accuracy: 0.0379\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4.6008 - accuracy: 0.0670 - val_loss: 4.5309 - val_accuracy: 0.0379\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4.4253 - accuracy: 0.0661 - val_loss: 4.3833 - val_accuracy: 0.0379\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.2726 - accuracy: 0.0722 - val_loss: 4.2577 - val_accuracy: 0.0379\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4.1432 - accuracy: 0.0696 - val_loss: 4.1527 - val_accuracy: 0.0379\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4.0353 - accuracy: 0.0617 - val_loss: 4.0682 - val_accuracy: 0.0379\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 3.9501 - accuracy: 0.0670 - val_loss: 4.0012 - val_accuracy: 0.0474\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.8845 - accuracy: 0.0678 - val_loss: 3.9486 - val_accuracy: 0.0521\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.8355 - accuracy: 0.0661 - val_loss: 3.9024 - val_accuracy: 0.0569\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.7954 - accuracy: 0.0600 - val_loss: 3.8600 - val_accuracy: 0.0569\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.7609 - accuracy: 0.0591 - val_loss: 3.8188 - val_accuracy: 0.0569\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.7284 - accuracy: 0.0583 - val_loss: 3.7818 - val_accuracy: 0.0521\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.6994 - accuracy: 0.0574 - val_loss: 3.7466 - val_accuracy: 0.0521\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.6709 - accuracy: 0.0591 - val_loss: 3.7163 - val_accuracy: 0.0521\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.6453 - accuracy: 0.0609 - val_loss: 3.6930 - val_accuracy: 0.0521\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.6239 - accuracy: 0.0574 - val_loss: 3.6779 - val_accuracy: 0.0474\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.6096 - accuracy: 0.0609 - val_loss: 3.6687 - val_accuracy: 0.0474\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.6001 - accuracy: 0.0696 - val_loss: 3.6599 - val_accuracy: 0.0474\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.5907 - accuracy: 0.0696 - val_loss: 3.6529 - val_accuracy: 0.0521\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.5836 - accuracy: 0.0722 - val_loss: 3.6451 - val_accuracy: 0.0427\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.5761 - accuracy: 0.0713 - val_loss: 3.6353 - val_accuracy: 0.0521\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5671 - accuracy: 0.0687 - val_loss: 3.6243 - val_accuracy: 0.0332\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5565 - accuracy: 0.0678 - val_loss: 3.6138 - val_accuracy: 0.0332\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5458 - accuracy: 0.0678 - val_loss: 3.6053 - val_accuracy: 0.0379\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5360 - accuracy: 0.0670 - val_loss: 3.6008 - val_accuracy: 0.0427\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5287 - accuracy: 0.0670 - val_loss: 3.5974 - val_accuracy: 0.0474\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5213 - accuracy: 0.0661 - val_loss: 3.5934 - val_accuracy: 0.0521\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5127 - accuracy: 0.0687 - val_loss: 3.5899 - val_accuracy: 0.0521\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.5044 - accuracy: 0.0704 - val_loss: 3.5867 - val_accuracy: 0.0474\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4966 - accuracy: 0.0687 - val_loss: 3.5833 - val_accuracy: 0.0474\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4895 - accuracy: 0.0678 - val_loss: 3.5789 - val_accuracy: 0.0474\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4825 - accuracy: 0.0670 - val_loss: 3.5752 - val_accuracy: 0.0474\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4778 - accuracy: 0.0696 - val_loss: 3.5714 - val_accuracy: 0.0474\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4740 - accuracy: 0.0704 - val_loss: 3.5677 - val_accuracy: 0.0427\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.4708 - accuracy: 0.0696 - val_loss: 3.5649 - val_accuracy: 0.0474\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.4686 - accuracy: 0.0687 - val_loss: 3.5624 - val_accuracy: 0.0521\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4663 - accuracy: 0.0678 - val_loss: 3.5587 - val_accuracy: 0.0521\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4620 - accuracy: 0.0670 - val_loss: 3.5569 - val_accuracy: 0.0521\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4588 - accuracy: 0.0713 - val_loss: 3.5559 - val_accuracy: 0.0474\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4563 - accuracy: 0.0722 - val_loss: 3.5542 - val_accuracy: 0.0474\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4532 - accuracy: 0.0722 - val_loss: 3.5522 - val_accuracy: 0.0427\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4501 - accuracy: 0.0730 - val_loss: 3.5503 - val_accuracy: 0.0427\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4472 - accuracy: 0.0713 - val_loss: 3.5478 - val_accuracy: 0.0427\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4440 - accuracy: 0.0713 - val_loss: 3.5468 - val_accuracy: 0.0474\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4433 - accuracy: 0.0722 - val_loss: 3.5454 - val_accuracy: 0.0474\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4426 - accuracy: 0.0730 - val_loss: 3.5421 - val_accuracy: 0.0474\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4402 - accuracy: 0.0739 - val_loss: 3.5379 - val_accuracy: 0.0521\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4374 - accuracy: 0.0730 - val_loss: 3.5349 - val_accuracy: 0.0521\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4356 - accuracy: 0.0722 - val_loss: 3.5324 - val_accuracy: 0.0521\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4341 - accuracy: 0.0687 - val_loss: 3.5303 - val_accuracy: 0.0521\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4326 - accuracy: 0.0687 - val_loss: 3.5291 - val_accuracy: 0.0521\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4315 - accuracy: 0.0696 - val_loss: 3.5271 - val_accuracy: 0.0521\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4300 - accuracy: 0.0696 - val_loss: 3.5248 - val_accuracy: 0.0521\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4278 - accuracy: 0.0722 - val_loss: 3.5237 - val_accuracy: 0.0521\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4270 - accuracy: 0.0722 - val_loss: 3.5230 - val_accuracy: 0.0521\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4267 - accuracy: 0.0730 - val_loss: 3.5213 - val_accuracy: 0.0521\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4248 - accuracy: 0.0757 - val_loss: 3.5202 - val_accuracy: 0.0474\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4233 - accuracy: 0.0730 - val_loss: 3.5208 - val_accuracy: 0.0474\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4230 - accuracy: 0.0730 - val_loss: 3.5210 - val_accuracy: 0.0427\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4219 - accuracy: 0.0730 - val_loss: 3.5218 - val_accuracy: 0.0427\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4215 - accuracy: 0.0713 - val_loss: 3.5215 - val_accuracy: 0.0474\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4210 - accuracy: 0.0722 - val_loss: 3.5199 - val_accuracy: 0.0474\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 3.4197 - accuracy: 0.0730 - val_loss: 3.5190 - val_accuracy: 0.0474\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4185 - accuracy: 0.0722 - val_loss: 3.5186 - val_accuracy: 0.0474\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4168 - accuracy: 0.0739 - val_loss: 3.5199 - val_accuracy: 0.0474\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4170 - accuracy: 0.0730 - val_loss: 3.5205 - val_accuracy: 0.0474\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4167 - accuracy: 0.0722 - val_loss: 3.5197 - val_accuracy: 0.0474\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4153 - accuracy: 0.0722 - val_loss: 3.5187 - val_accuracy: 0.0474\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4142 - accuracy: 0.0704 - val_loss: 3.5185 - val_accuracy: 0.0474\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4137 - accuracy: 0.0713 - val_loss: 3.5182 - val_accuracy: 0.0474\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4137 - accuracy: 0.0730 - val_loss: 3.5172 - val_accuracy: 0.0474\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4129 - accuracy: 0.0739 - val_loss: 3.5154 - val_accuracy: 0.0474\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4100 - accuracy: 0.0739 - val_loss: 3.5160 - val_accuracy: 0.0474\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4091 - accuracy: 0.0722 - val_loss: 3.5181 - val_accuracy: 0.0474\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4098 - accuracy: 0.0722 - val_loss: 3.5175 - val_accuracy: 0.0474\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4090 - accuracy: 0.0713 - val_loss: 3.5163 - val_accuracy: 0.0427\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4083 - accuracy: 0.0696 - val_loss: 3.5139 - val_accuracy: 0.0427\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.4067 - accuracy: 0.0722 - val_loss: 3.5131 - val_accuracy: 0.0474\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4058 - accuracy: 0.0713 - val_loss: 3.5130 - val_accuracy: 0.0474\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4049 - accuracy: 0.0722 - val_loss: 3.5139 - val_accuracy: 0.0474\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4050 - accuracy: 0.0713 - val_loss: 3.5136 - val_accuracy: 0.0474\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4043 - accuracy: 0.0713 - val_loss: 3.5124 - val_accuracy: 0.0474\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4034 - accuracy: 0.0713 - val_loss: 3.5126 - val_accuracy: 0.0474\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4027 - accuracy: 0.0713 - val_loss: 3.5130 - val_accuracy: 0.0474\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4017 - accuracy: 0.0713 - val_loss: 3.5139 - val_accuracy: 0.0427\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4015 - accuracy: 0.0704 - val_loss: 3.5139 - val_accuracy: 0.0427\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4010 - accuracy: 0.0713 - val_loss: 3.5127 - val_accuracy: 0.0427\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.3998 - accuracy: 0.0748 - val_loss: 3.5109 - val_accuracy: 0.0427\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.3984 - accuracy: 0.0757 - val_loss: 3.5119 - val_accuracy: 0.0427\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.3983 - accuracy: 0.0748 - val_loss: 3.5128 - val_accuracy: 0.0427\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.3976 - accuracy: 0.0748 - val_loss: 3.5140 - val_accuracy: 0.0427\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.3968 - accuracy: 0.0730 - val_loss: 3.5148 - val_accuracy: 0.0427\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.3962 - accuracy: 0.0704 - val_loss: 3.5142 - val_accuracy: 0.0427\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.3957 - accuracy: 0.0704 - val_loss: 3.5126 - val_accuracy: 0.0427\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3.3954 - accuracy: 0.0722 - val_loss: 3.5110 - val_accuracy: 0.0427\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.3942 - accuracy: 0.0730 - val_loss: 3.5107 - val_accuracy: 0.0427\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.3936 - accuracy: 0.0739 - val_loss: 3.5111 - val_accuracy: 0.0427\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.3929 - accuracy: 0.0748 - val_loss: 3.5132 - val_accuracy: 0.0474\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.3929 - accuracy: 0.0748 - val_loss: 3.5135 - val_accuracy: 0.0474\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.3926 - accuracy: 0.0748 - val_loss: 3.5129 - val_accuracy: 0.0474\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.3916 - accuracy: 0.0748 - val_loss: 3.5125 - val_accuracy: 0.0474\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.3907 - accuracy: 0.0739 - val_loss: 3.5130 - val_accuracy: 0.0474\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.3906 - accuracy: 0.0713 - val_loss: 3.5130 - val_accuracy: 0.0474\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.3902 - accuracy: 0.0722 - val_loss: 3.5125 - val_accuracy: 0.0427\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.3897 - accuracy: 0.0722 - val_loss: 3.5123 - val_accuracy: 0.0427\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 3.3887 - accuracy: 0.0730 - val_loss: 3.5124 - val_accuracy: 0.0474\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 825ms/step - loss: 6.7751 - accuracy: 0.0239 - val_loss: 6.3275 - val_accuracy: 0.0443\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.3166 - accuracy: 0.0416 - val_loss: 5.9479 - val_accuracy: 0.0296\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5.9248 - accuracy: 0.0505 - val_loss: 5.6214 - val_accuracy: 0.0345\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 5.5886 - accuracy: 0.0523 - val_loss: 5.3349 - val_accuracy: 0.0345\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 5.2941 - accuracy: 0.0558 - val_loss: 5.0794 - val_accuracy: 0.0345\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 5.0331 - accuracy: 0.0629 - val_loss: 4.8568 - val_accuracy: 0.0345\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.8072 - accuracy: 0.0602 - val_loss: 4.6611 - val_accuracy: 0.0443\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 4.6093 - accuracy: 0.0655 - val_loss: 4.4879 - val_accuracy: 0.0443\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 4.4359 - accuracy: 0.0655 - val_loss: 4.3357 - val_accuracy: 0.0345\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.2848 - accuracy: 0.0717 - val_loss: 4.2029 - val_accuracy: 0.0443\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.1548 - accuracy: 0.0762 - val_loss: 4.0885 - val_accuracy: 0.0296\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.0458 - accuracy: 0.0735 - val_loss: 3.9940 - val_accuracy: 0.0493\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.9584 - accuracy: 0.0717 - val_loss: 3.9190 - val_accuracy: 0.0345\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.8911 - accuracy: 0.0717 - val_loss: 3.8625 - val_accuracy: 0.0443\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.8416 - accuracy: 0.0709 - val_loss: 3.8162 - val_accuracy: 0.0493\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.8007 - accuracy: 0.0735 - val_loss: 3.7739 - val_accuracy: 0.0443\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.7630 - accuracy: 0.0691 - val_loss: 3.7369 - val_accuracy: 0.0493\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.7293 - accuracy: 0.0655 - val_loss: 3.7058 - val_accuracy: 0.0443\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.6997 - accuracy: 0.0638 - val_loss: 3.6791 - val_accuracy: 0.0443\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.6733 - accuracy: 0.0664 - val_loss: 3.6564 - val_accuracy: 0.0443\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.6491 - accuracy: 0.0682 - val_loss: 3.6392 - val_accuracy: 0.0394\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.6291 - accuracy: 0.0673 - val_loss: 3.6306 - val_accuracy: 0.0493\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.6177 - accuracy: 0.0673 - val_loss: 3.6235 - val_accuracy: 0.0493\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.6078 - accuracy: 0.0664 - val_loss: 3.6166 - val_accuracy: 0.0443\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5984 - accuracy: 0.0638 - val_loss: 3.6102 - val_accuracy: 0.0443\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5906 - accuracy: 0.0664 - val_loss: 3.6024 - val_accuracy: 0.0591\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5825 - accuracy: 0.0682 - val_loss: 3.5928 - val_accuracy: 0.0542\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5734 - accuracy: 0.0620 - val_loss: 3.5827 - val_accuracy: 0.0591\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5644 - accuracy: 0.0682 - val_loss: 3.5740 - val_accuracy: 0.0837\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.5561 - accuracy: 0.0717 - val_loss: 3.5651 - val_accuracy: 0.0591\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 3.5468 - accuracy: 0.0691 - val_loss: 3.5572 - val_accuracy: 0.0542\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 3.5373 - accuracy: 0.0709 - val_loss: 3.5500 - val_accuracy: 0.0443\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5271 - accuracy: 0.0682 - val_loss: 3.5449 - val_accuracy: 0.0296\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.5182 - accuracy: 0.0691 - val_loss: 3.5415 - val_accuracy: 0.0345\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.5103 - accuracy: 0.0682 - val_loss: 3.5395 - val_accuracy: 0.0345\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.5034 - accuracy: 0.0691 - val_loss: 3.5370 - val_accuracy: 0.0394\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4965 - accuracy: 0.0691 - val_loss: 3.5349 - val_accuracy: 0.0394\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4912 - accuracy: 0.0700 - val_loss: 3.5336 - val_accuracy: 0.0394\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4878 - accuracy: 0.0673 - val_loss: 3.5317 - val_accuracy: 0.0345\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4851 - accuracy: 0.0655 - val_loss: 3.5266 - val_accuracy: 0.0345\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4807 - accuracy: 0.0647 - val_loss: 3.5224 - val_accuracy: 0.0345\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4778 - accuracy: 0.0655 - val_loss: 3.5180 - val_accuracy: 0.0296\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4749 - accuracy: 0.0664 - val_loss: 3.5122 - val_accuracy: 0.0296\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4707 - accuracy: 0.0691 - val_loss: 3.5064 - val_accuracy: 0.0443\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4661 - accuracy: 0.0709 - val_loss: 3.5035 - val_accuracy: 0.0493\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4640 - accuracy: 0.0753 - val_loss: 3.5016 - val_accuracy: 0.0493\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.4626 - accuracy: 0.0726 - val_loss: 3.4987 - val_accuracy: 0.0493\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4598 - accuracy: 0.0735 - val_loss: 3.4957 - val_accuracy: 0.0394\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4569 - accuracy: 0.0709 - val_loss: 3.4931 - val_accuracy: 0.0345\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4549 - accuracy: 0.0717 - val_loss: 3.4907 - val_accuracy: 0.0345\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4532 - accuracy: 0.0744 - val_loss: 3.4883 - val_accuracy: 0.0345\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4514 - accuracy: 0.0726 - val_loss: 3.4860 - val_accuracy: 0.0296\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 3.4493 - accuracy: 0.0691 - val_loss: 3.4840 - val_accuracy: 0.0345\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4469 - accuracy: 0.0673 - val_loss: 3.4835 - val_accuracy: 0.0345\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4454 - accuracy: 0.0664 - val_loss: 3.4835 - val_accuracy: 0.0345\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4442 - accuracy: 0.0655 - val_loss: 3.4832 - val_accuracy: 0.0345\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4426 - accuracy: 0.0655 - val_loss: 3.4833 - val_accuracy: 0.0345\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4413 - accuracy: 0.0700 - val_loss: 3.4818 - val_accuracy: 0.0394\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4395 - accuracy: 0.0691 - val_loss: 3.4801 - val_accuracy: 0.0394\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4383 - accuracy: 0.0700 - val_loss: 3.4783 - val_accuracy: 0.0345\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.4372 - accuracy: 0.0709 - val_loss: 3.4774 - val_accuracy: 0.0345\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4366 - accuracy: 0.0691 - val_loss: 3.4755 - val_accuracy: 0.0345\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4347 - accuracy: 0.0682 - val_loss: 3.4739 - val_accuracy: 0.0345\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4330 - accuracy: 0.0682 - val_loss: 3.4743 - val_accuracy: 0.0345\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4323 - accuracy: 0.0682 - val_loss: 3.4753 - val_accuracy: 0.0345\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4320 - accuracy: 0.0709 - val_loss: 3.4756 - val_accuracy: 0.0345\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.4308 - accuracy: 0.0717 - val_loss: 3.4748 - val_accuracy: 0.0345\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4287 - accuracy: 0.0709 - val_loss: 3.4742 - val_accuracy: 0.0394\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4272 - accuracy: 0.0700 - val_loss: 3.4758 - val_accuracy: 0.0345\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4279 - accuracy: 0.0682 - val_loss: 3.4762 - val_accuracy: 0.0345\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.4274 - accuracy: 0.0691 - val_loss: 3.4752 - val_accuracy: 0.0345\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.4256 - accuracy: 0.0691 - val_loss: 3.4751 - val_accuracy: 0.0345\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3.4247 - accuracy: 0.0682 - val_loss: 3.4755 - val_accuracy: 0.0345\n",
      "OOS Log Loss: 3.4426650602865174\n"
     ]
    }
   ],
   "source": [
    "n_hidden_layers = study.best_params['n_hidden_layers']\n",
    "\n",
    "hidden_layer_neurons = []\n",
    "l1s = []\n",
    "l2s = []\n",
    "for i in range(n_hidden_layers):\n",
    "    hidden_layer_neurons.append(study.best_params[f\"hidden_layer_{i+1}_neurons\"])\n",
    "    l1s.append(study.best_params[f\"hidden_layer_{i+1}_l1\"])\n",
    "    l2s.append(study.best_params[f\"hidden_layer_{i+1}_l2\"])\n",
    "\n",
    "learning_rate = study.best_params[f\"learning_rate\"]\n",
    "batch_size = study.best_params[f\"batch_size\"]\n",
    "\n",
    "y_oos_list = []\n",
    "y_pred_list = []\n",
    "testing_data = []\n",
    "for X_train, X_val, X_oos, y_train, y_val, y_oos, df_mod_oos in cv_data:\n",
    "    # make sure to build mod in loop to prevent history\n",
    "    mod = build_and_compile_model(X_train.shape[1:], y_train.shape[1], hidden_layer_neurons, l1s, l2s, learning_rate)\n",
    "\n",
    "    mod.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=500,\n",
    "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    y_preds = mod.predict(X_oos, verbose=0)\n",
    "\n",
    "    df_preds = pd.DataFrame(\n",
    "        y_preds,\n",
    "        columns=y.columns\n",
    "    )\n",
    "\n",
    "    testing_data.append(pd.concat((df_mod_oos.reset_index(drop=True), df_preds), axis=1))\n",
    "\n",
    "\n",
    "    y_oos_list.append(y_oos)\n",
    "    y_pred_list.append(y_preds)\n",
    "\n",
    "df_test = pd.concat(testing_data, ignore_index=True)\n",
    "\n",
    "y_oos_concat = np.vstack(y_oos_list)\n",
    "y_pred_concat = np.vstack(y_pred_list)\n",
    "\n",
    "print(f\"OOS Log Loss: {log_loss(y_oos_concat, y_pred_concat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_distribution(row):\n",
    "    distribution = row[[f\"{i} Fantasy Points\" for i in range(-4, 56)]].astype('float64')/row[[f\"{i} Fantasy Points\" for i in range(-4, 56)]].astype('float64').sum()\n",
    "\n",
    "    return np.random.choice([i for i in range(-4, 56)], p=distribution, size=1_000)\n",
    "\n",
    "# get_random_distribution(df_test.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006982543640897756"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_test['Fantasy Points'] < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(22)\n",
    "\n",
    "row = df_test.iloc[-2]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "random_distribution = get_random_distribution(row)\n",
    "\n",
    "ax = sns.kdeplot(random_distribution, fill=True, bw_adjust=1.0, color='blue', label='Projected Fantasy Points Distribution')\n",
    "\n",
    "current_limits = ax.get_ylim()\n",
    "\n",
    "plt.vlines(x=row['Fantasy Points'], ymin=0.0, ymax=1.0, colors=['red'], label='Actual Fantasy Points Scored')\n",
    "\n",
    "ax.set_ylim(current_limits)\n",
    "\n",
    "plt.title(f\"{row['Name'].title()} {row['Season']} Week {row['Week']} Projected Fantasy Points Distribution\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.360638427268525"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "root_mean_squared_error(\n",
    "    df_test['Fantasy Points'],\n",
    "    np.dot(np.array([i for i in range(-4, 56)]), df_test[[f\"{i} Fantasy Points\" for i in range(-4, 56)]].transpose()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.866581880737527"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(\n",
    "    df_test['Fantasy Points'],\n",
    "    np.dot(np.array([i for i in range(-4, 56)]), df_test[[f\"{i} Fantasy Points\" for i in range(-4, 56)]].transpose()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often do players end up in certain percentiles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles = [(df_test.iloc[i]['Fantasy Points'] > get_random_distribution(df_test.iloc[i])).mean() for i in range(df_test.shape[0])]\n",
    "\n",
    "len(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(\n",
    "    x=percentiles,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2005.000000\n",
       "mean        0.496674\n",
       "std         0.286852\n",
       "min         0.000000\n",
       "1%          0.009000\n",
       "5%          0.052000\n",
       "10%         0.105000\n",
       "25%         0.245000\n",
       "50%         0.495000\n",
       "75%         0.748000\n",
       "90%         0.896000\n",
       "95%         0.946000\n",
       "99%         0.985960\n",
       "max         1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(percentiles).describe(percentiles=[0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
